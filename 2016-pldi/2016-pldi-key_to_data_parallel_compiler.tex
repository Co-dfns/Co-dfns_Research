\documentclass[pldi]{sigplanconf-pldi15}
% \usepackage[utf8]{inputenc}
\usepackage{fontspec}
\setmainfont[Ligatures=TeX]{STIX}
\setmonofont{APL385 Unicode}
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink
\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI

\begin{document}

\title{The Key to a Data Parallel Compiler}

\maketitle
\begin{abstract}
TBW
\end{abstract}

%[c]Proposed new outline:
%[c]
%[c]I. 	Generalization on manipulations over Sub-tree computation
%[c]  A.	Key + Encoding → Data parallel, Array-based computation
%[c]  B.	Easy to write/read
%[c]  C.	Uniform across passes
%[c]II.	Case Studies
%[c]  A.	Function Lifting
%[c]  B.	Expression Flattening
%[c]  C.	Lexical Scope Resolution
%[c]  D.	Illustrates the problem space
%[c]III.	Other passes/Issues
%[c]  A.	Discussion of other passes
%[c]  B.	Beyond the Key Operator
%[c]IV.	In practice/Co-dfns Compiler
%[c]V.	Performance/Results
%[c]
%[c]Questions that must be addressed:
%[c]
%[c]1. Why the formalization in APL?
%[c]2. Why parallelize a compiler?
%[c]3. How well does it work/scale/generalize

%[of]:Introduction
\section{Introduction}

Compilers are everywhere, but their design and structure does not scale well to modern 
parallel architectures. Traditional compilers are very difficult to parallelize, and the benefits of 
modern architectures is often seen only through the use of distributed build systems which invoke 
the single threaded compilers on multiple ``documents'' at the same time. Future CPU architectures 
promise to be even more data parallel, such as the Xeon Phi and the GPGPU architectures. On these 
systems it is even harder to obtain good performance for compilers written in the traditional fashion. 

Fundamentally, a compiler is a translator that operates over some sort of tree and transforms it to 
another tree of an usually radically different shape but equivalent ``semantics.'' These algorithms and 
transformations show up everywhere, from document processing on the Web, XML-based publishing 
workflows, to the programming language compilers with which most developers are intimately familar. 
These transformations differ from other widely studied parallel graph problems in that they tend to greatly 
alter the fundamental structure of the graph/tree, unlike, say, breadth-first search or single-source 
shortest path algorithms. This rapidly shifting graph structure requires different approaches to 
achieving good parallelism. 

There has been some interesting use of GPUs to accelerate certain compiler analyses and passes. 
Unfortunately, attempts to parallelize compilers usually takes the form of a great deal of work spent to 
highly optimize a single compiler pass. The results are often fragile and require a great deal of effort 
to maintain and keep in line with the rest of the compiler. This integration problem makes it difficult to 
scale the "ad hoc" parallelization effort of individual compiler passes to a complete system. This is 
particularly true in situations where the compiler passes may change rapidly, such as with XSLT or 
document publishing, where it is very common for the document preparer to write or tweak transformation 
passes to suit the specific class of documents being handled. In these cases, the expertise and time 
involved in parallelizing such transformations makes the ad hoc approach mostly intractable. 

Furthermore, compilers are increasingly becoming a part of a "live" and interactive feedback loop. 
Just in time compilation and on the fly static compilation are both used to provide custom binaries tuned 
to specific tasks on the fly. These techniques are used in web services and large scale databases, where 
in-memory databases may construct functions on the fly to execute. There is great interest in moving 
such systems on to the GPU or other highly data-parallel architectures, but it would significantly hamper 
the performance of the system if the compilation phase of query execution were required to occur on the 
CPU, especially at scale where there may be many more GPU accelerators than CPU resources. 

We propose a language-driven approach to compiler construction that provides a framework for the 
development of arbitrary compiler transformations that are data-parallel by construction. These 
techniques intentionally optimize for the data-parallel case, which permits a wide degree of platform 
independent performance compared to the traditional approach to compiler design, which requires 
significant effort and possibly signficiant transformation of the source code to achieve good parallel 
performance on high-parallel architectures. 

We use a mathematically oriented, array based language that is accessible and popular with information 
scientists. We restrict this language to permit only the composition of data-flow oriented array based 
programs whose core primitives are all data-parallel by definition. The permitted compositions of these 
primitives ensures that the resulting programs are both analyzable and parallel by construction, permitting 
a wide array of whole program and transformation specific optimizations that have been long studied. 
The notation is intentially close to mathematical notation, and as such, the resulting compiler passes 
more closely resemble the compact formal descriptions of the transformations rather than complex 
recursive algorithms for transforming trees. 

At the core of such techniques, we must have some method for transforming trees in a controlled and 
predictable manner. We build on the Nanopass paradigm for compiler construction, wherein a compiler is 
developed as a series of small, functional tree transformations (passes) that have an input AST language 
and an output AST language. Through progressive refinement of the tree, the composition of these 
passes results in the complete compiler. This is a highly functional approach, with very little mutation. 
Fundamentally, each pass must identify regions of the tree upon which it works, do some sort of computation 
on that sub-tree that has been matched (often through pattern matching on the nodes) and reassemble 
the tree in some new order that matches the desired structure. In other words, a compiler nanopass 
can be thought of as an arbitrary computation over a given selection of sub-trees in an AST, where 
this computation could move, relocate, or change the sub-tree. 

The key to our language-drive approach is a set of techniques that enable a data-parallel expression of 
these arbitrary transformations. By using a higher-order function called Key, together with basic array 
programming techniques and an encoding for the tree connections called a Node Coordinate matrix, we can 
identify sub-trees and select them for computation by their parent-child relationships in a data-parallel 
fashion, and then compute on these sub-trees using traditional array programming techniques. 

We have constructed a complete compiler for a superset of our language called Co-dfns. This compiler is 
part of a commercial effort to develop platform independent performance for array programs. The 
compiler is developed completely in a data-parallel by construction as described in this paper, and uses 
only traditional and well known array primitives together with function composition to form the entire 
core compiler passes and generator. As a result, the compiler takes the form of a pure data-flow graph 
without mutation and has an extremely uniform and concise definition. It uses no recursion, pattern 
matching, branching, or other forms of explicit looping. The compiler, together with its runtime libraries 
and parser, comes in under 1000 lines of code. 

In this paper we fully elucidate three compiler passes from the Co-dfns compiler to demonstrate the 
techniques used and we provide further analysis of how these techniques are used in the other compiler 
passes. We further provide a comparative analysis of these passes against their counterparts implemented 
using the Nanopass compiler framework, comparing performance both in terms of programmer effort and 
portability/maintainability, as well as runtime performance. 

\paragraph{Contributions}

\begin{itemize}[noitemsep]
\item We describe a method of computing over arbitrary sub-trees selected by their parent-child 
relationships in a data-parallel manner using the Key operator and Node Coordinates.
\item We situate this technique into a broader, language-driven compiler construction strategy 
that enables the development of parallel by construction compilers that exhibit desirable programmability 
features in terms of code size, complexity, portability, and maintainability. 
\item We provide analysis of these techniques as used in a commercial compiler project called Co-dfns and 
report on the results.
\item We compare these techniques to the Nanopass approach to compiler design in terms of programmer 
productivity and compiler runtime performance. 
\item We enumerate a set of programming techniques, patterns, and strategies that complement the 
Key operator and Node Coordinates in the construction of tree transformations in general and give some 
intuitions on the use of these techniques on arbitrary transformations.
\end{itemize}
%[cf]
%[of]:Generalized Sub-tree Computation
\section{Data Parallel Sub-tree Computation}

%[of]:The Key Operator
\subsection{The Key Operator}

The Key operator (written \verb;⌸;), takes a single function that expects two arguments and returns 
a function which takes two arguments. The left argument is a set of keys, and the right argument 
is a set of corresponding elements associated with those keys. In our case, we provide matrices 
for both of these arguments, so each pair of rows corresponds to a key-value pair. A simple example 
of the key operation at work is to compute a histogram (≢ here can be thought of as “tally” or count):

\begin{verbatim}
       ⊢X←?10⍴5
1 1 4 0 1 3 1 1 2 1
      X(⊣,(≢⊢))⌸X
1 6
4 1
0 1
3 1
2 1
\end{verbatim}

To understand a bit better how the Key operator applies its function, consider the function {⍺ ⍵} 
which returns the pair of its right and left arguments. If we apply it to the same value as above, we 
get the following:

\begin{verbatim}
      X{⍺ ⍵}⌸X
1  1 1 1 1 1 1 
4  4           
0  0           
3  3           
2  2
\end{verbatim}

In our case, we use either Fk or Ek as our keys applied to the corresponding AST. We also will drop off 
the first row in each AST using 1↓ since this node “contains” everything. In a complete AST this is 
usually the Module boundary node which contains the entire set of functions and values in the module.
%[cf]
%[of]:Encoding the AST
\subsection{Encoding the AST}

We represent the AST as a matrix of 3 columns and n rows, one for each node in the tree. The first column 
contains the inter-node relationships in the form of a depth vector. The second column is a vector of the 
node types, while the third contains the “value” of the node, such as the name in a variable. 

Parsing the examples above gives the following ASTs:

\begin{verbatim}
      F(f)              E(v)
       │         ┌───────┼──────┐
       E         E      P(÷)    E
     ┌─┴─┐  ┌────┼────┐    ┌────┼────┐
     F   A V(a) P(+) V(b) V(c) P(×) V(d)
     │   │
     E  N(7) 
 ┌───┼────┐
V(⍵) P(+) V(⍵)
\end{verbatim}

The depth vector for these trees we name Fd and Ed, respectively:

\begin{verbatim}
Fd←0 1 2 3 4 4 4 2 2 3
Ed←0 1 2 2 2 1 1 2 2 2
\end{verbatim}

The node types we call Ft and Et:

\begin{verbatim}
Ft←’FEFEVPVAN’
Et←’EEVPVPEVPV’
\end{verbatim}

And finally, we call the values vectors Fv and Ev:

\begin{verbatim}
Fv←’f’ 0 0 0 ‘⍵’ ‘+’ 0 7
Ev←’v’ 0 ‘a’ ‘+’ ‘b’ ‘÷’ 0 ‘c’ ‘×’ ‘d’
\end{verbatim}

We combine these to form the respective AST matrices. We write A,B to catenate arrays A and B along their 
last axes and ⍪A to turn a vector into a 1-column matrix. Thus, the two tree matrices are given by the 
following expressions:

\begin{verbatim}
      Fd,Ft,⍪Fv
0 F f
1 E 0
2 F 0
3 E 0
4 V ⍵
4 P +
4 V ⍵
2 A 0
3 N 7
      Ed,Et,Ev
0 E v
1 E 0
2 V a
2 P +
2 V b
1 P ÷
1 E 0
2 V c
2 P ×
2 V d
\end{verbatim}

Note especially that all inter-node information lives in the depth vector, but that this information requires 
non-local access as described in the previous section to use it. Constructing node coordinates fixes this issue.
%[cf]
%[of]:Node Coordinates
\subsection{Node Coordinates}

%[of]:A Definition of Node Coordinates
\subsubsection{A Definition of Node Coordinates}

Every node in an AST has a node coordinate, named because a coordinate is a precise location in a space. 
We can imagine all the nodes arranged inside some multi-dimensional space, leading to a specific set of 
node coordinates. Many such arrangements exist, of varying usefulness. In our case, any arrangement 
should allow us to answer the following questions about any two arbitrary nodes in a tree:

\begin{enumerate}[noitemsep]
\item Are the nodes the same?
\item Are they siblings?
\item Does one appear “earlier” in the tree?
\item Are they at the same depth?
\item Is one an ancestor of another?
\end{enumerate}

In short, we care about the relative position of each node in a tree relative to any other. We define a node 
coordinate as follows.

A node coordinate is a vector whose length is the depth of the tree. Its elements are natural numbers. 
The count of non-zero elements in the vector is equal to the depth of that node in the tree. All zero elements
appear after the non-zero elements. That is, a coordinate is zero-padded on the right. When ordered 
lexicographically, the nodes for each coordinate appear in order according to a depth-first pre-order 
traversal of the tree. Each coordinate uniquely identifies a single node. Every ancestor’s coordinate is a 
prefix of any child’s coordinate, ignoring zeros. From the above it follows that every node is lexicographically 
greater than its left sibling and differs from it by exactly one non-zero element, and that this element is the 
last non-zero element in the coordinate. 

To understand how we construct these coordinates we must consider how we represent the AST.
%[cf]
%[of]:Constructing Node Coordinates
\subsubsection{Constructing Node Coordinates}

In building the node coordinates, our goal is to build a matrix where each row is a node 
coordinate satisfying our previously described requirements. We end up with an n d 
shaped matrix where n is the number of rows and d the depth of the tree. As noted above, 
the depth vector contains all the necessary information, but in the wrong form. 

We write f⌿A to reduce the first axis of A using function f. Thus, +⌿V is the sum of the 
elements in vector V. The function x⌈y gives the maximum of its two arguments. So, the 
depth of each tree is given by the following:

\begin{verbatim}
      1+⌈⌿Fd
5
      1+⌈⌿Ed
3
\end{verbatim}

We can obtain the ordered sequence  by writing \verb;⍳n;:

\begin{verbatim}
      ⍳3
0 1 2
\end{verbatim}

So the depths of all nodes that appear in the depth vectors is thus:

\begin{verbatim}
      ⍳1+⌈⌿Fd
0 1 2 3 4 5
      ⍳1+⌈⌿Ed
0 1 2
\end{verbatim}

The function table or outer product of f over vectors U and V is written U ∘.f V giving a U V 
shaped matrix as a result. Thus, (⍳3)∘.×⍳3 gives a small multiplication table:

\begin{verbatim}
      (⍳3)∘.×⍳3
0 0 0
0 1 2
0 2 4
\end{verbatim}

If we use = instead, we have a Boolean identity matrix:

\begin{verbatim}
      (⍳3)∘.=⍳3
1 0 0
0 1 0
0 0 1
\end{verbatim}

If we use ∘.= on the depth vector and its set of depths instead, we see an expanded Boolean 
representation of the depth vector:

\begin{verbatim}
      Fd∘.=⍳1+⌈⌿Fd
1 0 0 0 0
0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1
0 0 0 0 1
0 0 0 0 1
0 0 1 0 0
0 0 0 1 0
      Ed∘.=⍳1+⌈⌿Ed
1 0 0
0 1 0
0 0 1
0 0 1
0 0 1
0 1 0
0 1 0
0 0 1
0 0 1
0 0 1
\end{verbatim}

These matrices let us see the nesting features of each tree more visually, but also 
suggest another step. We can compute a sum scan with +⍀, also called a prefix sum, 
along the first axis. Applying this function on the above matrices leads to an interesting 
result:

\begin{verbatim}
      +⍀Fd∘.=⍳1+⌈⌿Fd
1 0 0 0 0
1 1 0 0 0
1 1 1 0 0
1 1 1 1 0
1 1 1 1 1
1 1 1 1 2
1 1 1 1 3
1 1 2 1 3
1 1 2 2 3
      +⍀Ed∘.=⍳1+⌈⌿Ed
1 0 0
1 1 0
1 1 1
1 1 2
1 1 3
1 2 3
1 3 3
1 3 4
1 3 5
1 3 6
\end{verbatim}

These matrices are lexicographically ordered, and each ancestor shares a common prefix 
with its descendants. They are also unique coordinates. Only the spurious digits at the end 
of each coordinate prevent these matrices from meeting all our requirements for valid node 
coordinates. 

The expression V f⍤¯1⊢M applies f to corresponding elements of V and rows of M:

\begin{verbatim}
      3 3⍴⍳9
0 1 2
3 4 5
6 7 8
      (⍳3)+⍤¯1⊢3 3⍴⍳9
0 1  2
4 5  6
8 9 10
\end{verbatim}

If n↑V takes the first n elements of V, then we can obtain coordinate matrices from the 
prefix sums by noting that the spurious digits all come after column d+1 where d is the 
depth corresponding to that coordinate. The following gives a complete expression for 
computing a node coordinate matrix from a depth vector, shown using Fd and Fe:

\begin{verbatim}
      ⊢Fc←(1+Fd)↑⍤¯1⊢+⍀Fd∘.=⍳1+⌈⌿Fd
1 0 0 0 0
1 1 0 0 0
1 1 1 0 0
1 1 1 1 0
1 1 1 1 1
1 1 1 1 2
1 1 1 1 3
1 1 2 0 0
1 1 2 2 0
      ⊢Ec←(1+Ed)↑⍤¯1⊢+⍀Ed∘.=⍳1+⌈⌿Ed
1 0 0
1 1 0
1 1 1
1 1 2
1 1 3
1 2 0
1 3 0
1 3 4
1 3 5
1 3 6
\end{verbatim}

A careful study of the definition of a node coordinate and the above construction should 
reveal why this works. Intuitively, we are creating a multi-dimensional space or a number 
system in which each digit place or dimension contains or circumscribes a smaller space in 
which are contained all the descendant nodes that appear lower in the tree. Each coordinate 
is a sort of special path through the tree encoded to have desirable properties relative to 
other paths.
%[cf]
%[of]:Operations on Node Coordinates
\subsubsection{Operations on Node Coordinates}

The simplest operation over a node coordinate is to extract the depth of the node. The 
expression C⍳0 finds the first occurrence of 0 in C and returns the index of that 
occurrence. (The application of ⍳ here is its dyadic form, meaning that it receives two 
arguments. Previous uses of ⍳ have used its monadic form, which generates all indices, 
instead of finding a specific index. Most APL functions have a dyadic and monadic form.) 
Thus, the depth of a node is given by the following:

\begin{verbatim}
      C←1 1 2 0 0
      ¯1+C⍳0
2
\end{verbatim}

The primary calculation we care about in order to do function lifting, expression flattening, 
and other sorts of flattening transformations is to determine which nodes are ancestors of 
another node. The core of this computation determines whether one node is a child of another 
based on their node coordinates. This amounts to computing whether one node is a prefix of 
another, ignoring zeros.

We write (f g h) to represent the composition of functions f, g, and h as a function train. This i
s used with another shorthand of composition according to the following equivalences:

\begin{verbatim}
A(f g h)B ←→ (A f B) g (A h B)
A(0 f h)B ←→ 0 f (A h B) ⍝ Any constant for 0
A(f g)B   ←→ f (A g B)
\end{verbatim}

With this, we can write a function to compute whether a given coordinate is a prefix of another.

\begin{verbatim}
      P←1 1 0 0 0
      C(=∨0=⊢)P
1 1 1 1 1
      ∧⌿C(=∨0=⊢)P
1
\end{verbatim}

Here P is the coordinate we wish to check against C, to determine whether C is a descendant. The
 functions are all standard logical functions extended to apply point-wise over arrays. The 
 function ⊢ always returns its right argument. The function (=∨0=⊢) reads intuitively as, “equal or 
 right element is zero.” In this case to make this a predicate we combine this with the “for all” 
 reduction using ∧⌿. This particular pattern is a special case of inner product, which we can compute 
 using f.g. Thus, +.× is a function that computes traditional matrix-matrix multiplication when applied 
 to two matrices. However, we can also use it to compute the above reduction and application:
 
\begin{verbatim}
      C∧.(=∨0=⊢)P
1
\end{verbatim}

This extends the reduction to allow us to use matrix values for C and P instead of vectors, and thus 
determine whether rows in C are descendants of multiple candidates given by P. This becomes important 
to using the prefix function. In our examples, we want to determine the nearest ancestor of a certain 
node type to which a node belongs. In the case of function lifting, we want to determine the nearest 
ancestor function, in the case of expression flattening, we want to determine the nearest ancestor 
expression node. We can extract the rows we care about for each example using ⌷, where i⌷M returns 
the i-th row of M. In the following expressions, we use 0 2 rather than ¯1 in ⍤ to indicate that ⌷ will 
receive the entire matrix on the right for each scalar index on the left.

\begin{verbatim}
      ⊢Fp←0 2⌷⍤0 2⊢Fc
1 0 0 0 0
1 1 1 0 0
      ⊢Ep←0 1 6⌷⍤0 2⊢Ec
1 0 0
1 1 0
1 3 0
\end{verbatim}

The matrices Fp and Ep correspond to the function and expression node coordinates in our two 
examples, respectively. We could now use Fp and Ep to compare against Fc and Ec and determine 
which nodes contained which nodes in the tree. However, our prefix function will return 1 when we 
ask whether a node is a prefix of itself. In this case we don’t want that. The solution to this is to 
drop the last non-zero element from the coordinate. This will not prevent it from matching against 
any of its ancestors, but will prevent matching against itself. We can use the depth vector with the 
↑ function to take all but the last non-zero element. We need to remember to extend the returned 
array with an extra zero column, since the shape will be too small otherwise. 

\begin{verbatim}
      ⊢Fcp←(Fd↑⍤¯1⊢Fc),0
0 0 0 0 0
1 0 0 0 0
1 1 0 0 0
1 1 1 0 0
1 1 1 1 0
1 1 1 1 0
1 1 1 1 0
1 1 0 0 0
1 1 2 0 0
      ⊢Ecp←(Ed↑⍤¯1⊢Ec),0
0 0 0
1 0 0
1 1 0
1 1 0
1 1 0
1 0 0
1 0 0
1 3 0
1 3 0
1 3 0
\end{verbatim}

We can now use Fcp and Ecp to determine which Function and Expression nodes match for each node:

\begin{verbatim}
     Fcp∧.(=∨0=⊢)⍉Fp
0 0
1 0
1 0
1 1
1 1
1 1
1 1
1 0
1 0
      Ecp∧.(=∨0=⊢)⍉Ep
0 0 0
1 0 0
1 1 0
1 1 0
1 1 0
1 0 0
1 0 0
1 0 1
1 0 1
1 0 1
\end{verbatim}

We use the ⍉ function, which computes the transpose of its right argument, to ensure that the row 
and column sizes match up. With the above, we can determine the “greatest” match, which is the closest. 
We can replace each 1 in the above matrices by their column numbers (the number of the column in which 
that 1 occurs), and then we can take the maximum column number in each row to determine an index in 
either Fp or Ep that is the appropriate “parent” coordinate for that node. This works because we have 
made sure to order the Ep and Fp matrices lexicographically. The following shows this computation for 
the expression example; we elide the function example as redundant at this point. We also use ⌈/ instead 
of ⌈⌿ to reduce along the last, rather than the first, axis. 

\begin{verbatim}
      ⊢Ei←⌈/(⍳3)×⍤1⊢Ecp∧.(=∨0=⊢)⍉Ep
0 0 1 1 1 0 0 2 2 2
      ⊢Ek←Ei⌷⍤0 2⊢Ep 
1 0 0
1 0 0
1 1 0
1 1 0
1 1 0
1 0 0
1 0 0
1 3 0
1 3 0
1 3 0
\end{verbatim}

At this point we have two values, Ek and Fk, which indicate the closest containing node that we care about 
for each node in the tree, using its node coordinate. 

\begin{verbatim}
      Fk
1 0 0 0 0
1 0 0 0 0
1 0 0 0 0
1 1 1 0 0
1 1 1 0 0
1 1 1 0 0
1 1 1 0 0
1 0 0 0 0
1 0 0 0 0
\end{verbatim}

We will use these keys to perform computation over the AST and accomplish our tasks of function lifting and 
expression flattening in the next section, but we make a final note here, that we can imagine many other 
operations which might be used throughout the compiler when we care about how nodes relate to each 
other. The use of operators like reduction, scan, inner and outer products allow us to obtain the necessary 
information from the node coordinate matrix. Sometimes these results might not be obvious, but the 
example of the “belongs to” relationship given above demonstrates a pattern that arises repeatedly 
throughout our data parallel compiler, and is therefore particularly useful to understand. 
%[cf]

%[cf]
%[cf]
%[of]:Case Studies
\section{Case Studies}

%[of]:Table of Compiler Passes
\begin{table*}
\centering
\begin{tabular}{l p{2.4in} l l l}
\textbf{Pass} & \textbf{Description} & \textbf{Core Patterns} 
 & \textbf{Key?} & \textbf{Coordinate?} \\ \hline
Record Node Coordinates & Adds a new field to each node containing that node's 
 node coordinate. & Outer Product/Scan & No & Yes \\
Record Function Depths & Adds a new field to each node recording how many 
 functions surround the node. & Inner Product & No & Yes \\
Drop Unnamed Functions & Eliminates some code that will not be evaluated at 
 the top-level. & Filter & No & No \\
Drop Unreachable Code & Eliminates some unreachable code. &
 Filter/Inner Product & No & Yes \\
Lift Functions & Moves all functions to the top-level. & Key & Yes & Yes \\
Drop Redundant Nodes & Eliminates unnecessary nodes/nesting & Filter & No & No \\
Flatten Expressions & Removes nesting from expressions. & Key & Yes & Yes \\
Compress Atomic Nodes & Atomizes nested atom nodes & Amend & No & No \\
Propagate Constants & Inlines all references to literal values. &
 Amend/Rank & No & Yes \\
Fold Constants & Converts constant expressions to literals & Amend & No & No \\
Compress Expressions & Converts expression sub-trees into single nodes. & 
 Amend & No & No \\
Record Final Return Value & Records the value returned by each function. & 
 Key & Yes & No \\
Normalize Values Field & Normalizes the shape and size of the values field. &
 Amend & No & No \\
Lift Type-checking & Infers some type information at compile time &
 Power Limit/Rank & No & No \\
Allocate Value Slots & Does a form of frame allocation for variables &
 Key & Yes & No \\
Anchor Variables & Resolves lexically scoped variables &
 Key & Yes & No \\
Record Live Variables & Records the variables that are live at each point of 
 execution. & Key & Yes & No \\
Fuse Scalar Loops & Identifies Fusion opportunities and fuses expressions &
 Key & Yes & No \\
Type Specialization & Specializes each function for a series of potential inputs. 
 & Key & Yes & No \\
\end{tabular}
\caption{A listing of some compiler passes in the Co-dfns compiler and their 
 relationship with the Key operator and associated tree computation techniques}
\label{table:passes}
\end{table*}
%[cf]
%[of]:Function Lifting
\subsection{Function Lifting}

If we use the key operator with Fk, we get the following (the monadic use of ↓ converts Fc from a 
matrix to a vector of vectors):

\begin{verbatim}
      (1↓Fk){⍺ ⍵}⌸1↓Fd,Ft,Fv,⍪↓Fc
┌─────────┬─────────────────┐
│1 0 0 0 0│┌─┬─┬─┬─────────┐│
│         ││1│E│0│1 1 0 0 0││
│         │├─┼─┼─┼─────────┤│
│         ││2│F│0│1 1 1 0 0││
│         │├─┼─┼─┼─────────┤│
│         ││2│A│0│1 1 2 0 0││
│         │├─┼─┼─┼─────────┤│
│         ││3│N│7│1 1 2 2 0││
│         │└─┴─┴─┴─────────┘│
├─────────┼─────────────────┤
│1 1 1 0 0│┌─┬─┬─┬─────────┐│
│         ││3│E│0│1 1 1 1 0││
│         │├─┼─┼─┼─────────┤│
│         ││4│V│⍵│1 1 1 1 1││
│         │├─┼─┼─┼─────────┤│
│         ││4│P│+│1 1 1 1 2││
│         │├─┼─┼─┼─────────┤│
│         ││4│V│⍵│1 1 1 1 3││
│         │└─┴─┴─┴─────────┘│
└─────────┴─────────────────┘
\end{verbatim}

Notice that we have now grouped all of the relevant parts of the tree according to which nodes would 
appear in their respective functions after lifting. Refer to the original lifting example in the 
introduction to verify this. Indeed. The second row in the above example shows the internal function 
complete and ready to name. Each element in the second column corresponds to the body of one of our 
lifted functions. In the case of the first function, the outer function, we have a spurious function node 
in the body. This is intentional. When we lift these functions, we will replace each spurious function 
node with a variable node referring to the function’s generated name. 

Each of these function bodies has a specific coordinate associated with it. Because these coordinates 
are uniquely identifying, we can use these as input into a name generator to generate names that we 
know are unique for each function body. Furthermore, because we retain this information in the 
corresponding function nodes that appear in the body of each function to be lifted, we know exactly what 
name that function has been given, and we can replace the function node with a variable node referencing 
that name instead, without referring to any state outside of the immediate information given to the 
function lifter. Indeed, each row in the above matrix represents a function lifting task that can be 
completed without any additional information. That is, there are no dependencies between rows to perform 
lifting. This gives us a straightforward parallel execution of function lifting. 

The final task to complete function lifting of each function body before lifting is to shift the depths in 
the depth vectors to correspond to those of a function lifted to the top-level, and to attach a function 
node to the top of each of the bodies. At that point, we simply recombine all of the newly created functions 
into a single top level. 
%[cf]
%[of]:Expression Flattening
\subsection{Expression Flattening}

If we use Ek as the key for the expression example, we get the following:

\begin{verbatim}
      (1↓Ek){⍺ ⍵}⌸1↓Ed,Et,Ev,⍪↓Ec
┌─────┬─────────────┐
│1 0 0│┌─┬─┬─┬─────┐│
│     ││1│E│0│1 1 0││
│     │├─┼─┼─┼─────┤│
│     ││1│P│÷│1 2 0││
│     │├─┼─┼─┼─────┤│
│     ││1│E│0│1 3 0││
│     │└─┴─┴─┴─────┘│
├─────┼─────────────┤
│1 1 0│┌─┬─┬─┬─────┐│
│     ││2│V│a│1 1 1││
│     │├─┼─┼─┼─────┤│
│     ││2│P│+│1 1 2││
│     │├─┼─┼─┼─────┤│
│     ││2│V│b│1 1 3││
│     │└─┴─┴─┴─────┘│
├─────┼─────────────┤
│1 3 0│┌─┬─┬─┬─────┐│
│     ││2│V│c│1 3 4││
│     │├─┼─┼─┼─────┤│
│     ││2│P│×│1 3 5││
│     │├─┼─┼─┼─────┤│
│     ││2│V│d│1 3 6││
│     │└─┴─┴─┴─────┘│
└─────┴─────────────┘
\end{verbatim}

Again, we can see immediately that we have grouped each set of nodes according to the expressions 
that are to be lifted. Just as in the case of function lifting, we can adjust the depths of each 
expression to the correct depth and we can replace each expression node with a variable reference
 based on that node’s coordinate. Each expression can be given a unique name based on its 
 coordinate. A later compiler pass can reduce these names down to the minimum actually required to 
 represent the expression if desired. 

The only extra issue involved here is to ensure that the order of evaluation matches. In our case, we 
are assuming that the order of evaluation is right to left, which means that the above order is actually 
backwards of the desired order. During recombination, we simply reverse these orders and this fixes 
that problem. More work would be required to take into consideration a specific precedence hierarchy. 
%[cf]
%[of]:Lexical Scoping
\subsection{Lexical Scoping}
%[cf]
%[cf]
%[of]:Other Passes
\section{Other Passes}
%[cf]
%[of]:The Co-dfns Compiler
\section{The Co-dfns Compiler}
%[cf]
%[of]:Performance
\section{Performance}
%[cf]
%[of]:Future Work
\section{Future Work}
%[cf]
%[of]:Related Work
\section{Related Work}

The J programming language [18] was the first practical, general-purpose programming language 
to introduce the key operator as a primitive operator with the presumption of its general 
usefulness. The rank operator (⍤) used throughout this treatment also derives from the J 
traditional, receiving particular interest throughout the APL community [1, 17]. 

Fritz Henglein demonstrated a class of operations, called discriminators, of which the Key operator 
is a member. [16] Namely, a discriminator performs the same grouping computation as Key, but does 
not apply a function over these groups with their keys. Henglein provides a linear implementation 
of these operations. 

The EigenCFA effort [22] demonstrated significant performance improvements of a 0-CFA flow 
analysis by utilizing similar techniques to those demonstrated here. In particular, encoding the 
AST and using accessor functions have a very similar feel to the node coordinates and AST 
encoding given here, though they have a different formulation and spend considerable effort 
understanding the trade-offs of performance associated with the different encodings, whereas 
the encodings here were chosen for their clarity and directness, rather than their performance. 

Mendez-Lojo, et al. implemented a GPU version of Inclusion-based Points-to Analysis [20] that also 
focuses on adapting data structures and algorithms to efficiently execute on the GPU. In particular, 
they use similar techniques of prefix sums and sorts to achieve some of their adaptation to the GPU, 
Additionally, they have clever and efficient methods of representing graphs on the GPU which enable 
dynamic rewriting of the graph. 

The APEX compiler [2] developed vectorized approaches to handling certain analyses to compile 
traditional APL, including a SIMD tokenizer [4]. It uses a SSA representation, and converts the
dynamic scope of traditional APL functions into a static form early on. It also uses a matrix 
format to represent the AST. Traditional APL did not have nested function definitions, however, 
and thus the APEX compiler does not have any specific approaches to dealing with function lifting.

Bernecky further identified methods of reducing or optimizing the computational complexity or cost 
of certain array operations, allowing improved performance of easy to understand array expressions [3].

Timothy Budd implemented a compiler [5, 6] for APL which targeted vector processors as well as one 
for C code. They used a method of lazy evaluation to avoid intermediate data copying. Budd provided 
thoughts and some ideas on how the compiler might be implemented in parallel as well. 

Walter Schwarz implemented an APL to C compiler for the ACORN system targeting the CM-2 machine, 
demonstrating performance potential for APL as a massively parallel language [21].

W. Ching and D. Ju have spent significant work on the ELI language and other APL-class language
 implementations, especially on parallelized code and optimization. [8, 9, 10, 11, 13, 14, 15]

J. D. Bunda and J. A. Gerth presented a method for doing table driven parsing of APL which 
suggested a parallel optimization for parsing, but did not elucidate the algorithm [7].
%[cf]
%[of]:Conclusion
\section{Conclusion}

We have derived a method of performing computation over sub-trees selected on the basis of 
inter-node properties through the use of the Key (⌸) operator and node coordinates, which enable 
local computation of these inter-node properties. This method is both general and direct, and when 
combined with traditional and more mundane array programming, suffices to implement the 
complete core of a compiler, modulo parsing and code generation. The method requires no special 
operations or unique special casing primitives in the language. Moreover, it is strictly data-parallel 
and data-flow, without any complex control flow, which not only results in concise code, but also 
enables much easier analysis of properties such as runtime complexity and verification of correctness. 

We have demonstrated the technique and the core insights behind the data structures involved. It 
presents a solution to a very old and traditional problem in a very uncommon light, by eschewing 
the common practices that underlie every other significant and general solution found in modern 
compilers today and replacing them with an entirely different paradigm centred on parallelism 
and aggregate operations.
%[cf]
%[of]:Acknowledgements
%[cf]

%[of]:Unintegrated Sections
\section{Unintegrated Sections}

%[of]:Scaling up, Considerations and Thoughts
\subsection{Scaling up, Considerations and Thoughts}

How well do these techniques work in practice? We have applied these techniques to an entire compiler, 
with the same restrictions as used here. The result was a success, and the core of the compiler, which 
is everything but parsing and code generation, is contained in a single file consisting of only 40 lines. 
In 12 point font, the compiler easily fits on a single sheet of paper. It produces C and CUDA code.

Not every pass in the compiler requires this sort of operation. Indeed, if flattening occurs early and 
in the correct order, it’s possible to use much simpler methods of grouping using only a prefix sum 
scan instead of operating over the node coordinates. It’s also interesting to note that we do not 
need to recompute the node coordinates throughout the compiler. Indeed, this would defeat their 
purpose somewhat, as the node coordinates allow us to understand the original position of a node, 
even if that node has now been lifted elsewhere. We use this information when resolving lexical scope, 
for instance. 

When writing data-parallel code, duplicated work is common. The existing compiler contains little to 
no inherently duplicated work. Most of the duplicated work exists for brevity and clarity of the code, 
rather than for any required reason, and could easily be factored out. 

Finally, it is interesting to note that the simplified control flow gives rise to some positive side-effects. 
Besides brevity and directness, the complexity of each operation and the composition of operations is 
well known. We have not yet tried to do this, but it is straightforward to derive a worst-case complexity 
bound on code written in this style, so simple, in fact, that it can be accomplished automatically. It is 
conceivable then, that the compiler could produce not only an executable program from code written 
in this style, but also a complexity bound on its execution. 
%[cf]
%[of]:Computing with Node Coordinates
\subsection{Computing with Node Coordinates}

Given the above keys and grouping information, we now have all the information we need to do 
function lifting and expression flattening, but we lack a suitable operator to complete the task. 
Indeed, it isn’t immediately apparent that a single, general purpose operator over arrays exists 
which can accomplish such a task. However, the Key operator, while masquerading as a useful business 
analytics operator, represents just such a useful tree transformation tool. 
%[cf]
%[of]:Introduction (Old)
\subsection{Introduction (Old)}

When compiling a high-level language into a low-level language, we often implement a variety of 
flattening transformations to normalize more complex code exhibiting nesting into flat programs 
that are semantically equivalent. Function lifting, for example, takes a program with nested functions 
into an equivalent program in which all functions are at the root of the program. The following function
f always returns 14.

\begin{verbatim}
f←λ.(λ ⍵.(⍵+⍵))7
\end{verbatim}

A function lifting pass would transform this into the following:

\begin{verbatim}
fn1←λ⍵.⍵+⍵
f←λ.fn1 7
\end{verbatim}

Expression flattening works in a similar fasion. Assuming all expressions evaluate right to left, this expression:

\begin{verbatim}
v←(a+b)÷c×d
\end{verbatim}

Becomes this:

\begin{verbatim}
v←c×d
t←a+b
v←t÷v
\end{verbatim}

Characteristic of many compiler passes, most of the work is manipulating the tree, rather than, say, number 
crunching. Almost universally, compilers use a combination of recursion and branching to implement these 
transformations, such as using the visitor pattern, Nanopass framework [19], or direct structural recursion. 
The “work” happens in the control flow of the program text. The ubiquity of this approach warrants note. 
Let us savour such rare solidarity for a moment. Now, let us challenge it. 

Let us arbitrarily restrict ourselves to a data-flow, data-parallel model. Specifically, consider a language 
whose core primitives are pointwise functions over n-dimensional arrays. We have a set of operators that 
take functions and apply them over different sub-parts of arrays. We can compose these functions together. 
That is all. Can we implement a compiler in such a language? The obvious answer is, yes, given arbitrary 
primitives. What if we restrict ourselves to standard and general-purpose primitives? The answer is still 
yes. Furthermore, the method is surprisingly direct and concise. The language we use is a subset of Dyalog 
APL 14.0 and the reader may follow the examples in the following sections using TryAPL.org. 

The trick to such a compiler is solving the issue illustrated by the previous examples, expressing tree 
manipulation without embedding the core logic in the control flow. We accomplish this by choosing an 
appropriate representation of the AST and a property of each node we call its node coordinate. These 
coordinates allow us to locally answer the inter-node questions we need without complex control flow. When 
we combine this with a general-purpose operator called “Key,” we can perform arbitrary computation over 
sub-trees that we partition based on these inter-node relationships, such as whether one node is an ancestor 
of another. When combined with more mundane array programming, we can implement the complete core of a 
compiler, including lexical scope, function lifting, expression flattening/normalization, loop fusion, 
frame/register allocation, and so on. The result is a completely data-parallel compiler modulo parsing and 
some aspects of code generation.

The notation used throughout this paper is introduced piecemeal as necessary in each relevant section in 
which the notation is first used. All functions are applied infix and take either one or two arguments. All 
expressions evaluate from right to left. Tables 1 and 2 provide a reference for the notation and primitives 
as a reference aid.

%[of]:Contributions
\paragraph{Contributions}

\begin{itemize}[noitemsep]
\item We introduce node coordinates as a means to locally compute inter-node properties without using complex 
control flow.
\item We demonstrate methods to perform arbitrary computation over sub-trees partitioned by these 
inter-node properties in a pure data-flow, data-parallel style.
\item The presented methods are direct and concise; the data structures, simple.
\item These methods require no specialized operations, but instead, use only established, general-purpose 
array programming.
\end{itemize}
%[cf]
%[cf]

%[cf]

% Bibliography goes here

\end{document}