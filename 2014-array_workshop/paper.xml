<?xml version="1.0" encoding="utf-8" ?>

<article xmlns="http://docbook.org/ns/docbook" version="5.0">
  <info>
    <title>Co-dfns: Ancient Language, Modern Compiler</title>
    <authorgroup>
      <author>
        <personname>Aaron W. Hsu</personname>
        <affiliation><orgname>Indiana University</orgname></affiliation>
        <email>awhsu@indiana.edu</email>
      </author>
    </authorgroup>
    <date>Thursday, March 13th, 2014</date>
    <abstract>
      <para>
        The APL language allows subject matter experts with no 
        computer science experience to create large and complex 
        software implementations of ideas without excessive 
        software engineering and external development costs.
        The rapid increase in data sizes challenges existing 
        APL systems, which are all interpreters, to scale with 
        the domain expert's problem size. The Co-dfns project, 
        currently in early development stages, focuses on 
        delivering new language innovations and a high-performance 
        compiler for modern APL that enables domain experts to scale 
        their ideas to their data using modern hardware architectures.
        The Co-dfns language provides concise primitives that 
        allow domain experts to utilize concurrency and explicit 
        task parallelism in their code. The compiler addresses
        critical performance bottlenecks of the Dyalog APL interpreter,
        while maintaining strong integration and compatibility with existing 
        APL workflows. Co-dfns will allow domain experts to scale 
        their code more successfully without requiring them to 
        rewrite critical portions of their code in other languages. 
        Early benchmarking efforts indicate that even naive and 
        unoptimized versions of the Co-dfns compiler compete well 
        with Dyalog on code that naturally lends itself to APL.
      </para>
    </abstract>
    <legalnotice>
      <para>
        Permission to make digital or hard copies of all or part of this
        work for personal or classroom use is granted without fee 
        provided that copies are not made or distributed for profit or 
        commercial advantage and that copies bear this notice and the 
        full citation on the first page. Copyrights for components of 
        this work owned by others than the author(s) must be honored. 
        Abstracting with credit is permitted. To copy otherwise, or 
        republish, to post on servers or to redistribute to lists, 
        requires prior specific permission and/or a fee. Request 
        permissions from Permissions@acm.org.
      </para>
      <para>
        <literallayout><citetitle>ARRAY'14</citetitle>, June 11 2014, Edingburgh, United Kingdom
Copyright is held by the owner/author(s). Publication rights licensed to ACM
ACM 978-1-4503-2937-8/14/06...$15.00
http://dx.doi.org/10.1145/2627373.2627384</literallayout>
      </para>
    </legalnotice>
  </info>
  <section>
    <title>Introduction</title>
    <para>
      The general purpose array languages, especially APL, represent a 
      critical business tool for domain experts. These experts 
      often rely on APL to construct large software systems oriented around 
      their particular expertise. Unlike traditional computer scientists, 
      the domain expert primarily solves problems outside of computing;
      APL provides agility and low-cost
      by reducing the domain expert's dependence on 
      external engineering resources. This low-overhead, direct engagement 
      with the programming environment distinguishes APL's primary successes 
      from languages such as C++ or C. <citation>mortenemail</citation>
    </para>
    <para>
      The systems developed in this manner could be small, but often 
      comprise large systems over large data. 
      This includes treasury management, risk 
      and portfolio management, oil refining optimization, and large 
      installations of patient journals and health data. 
      <citation>mortenemail</citation> Increasingly, 
      in the fields where APL thrives, the size of data continues to expand rapidly. 
      These problems often exceed the ability of even large 
      machines to store all relevant information in RAM, or even on a single 
      machine. <citation>cosmos</citation>
    </para>
    <para>
      This rapid expansion in program and data sizes pushes the current APL
      implementations. The success 
      of these programs hinges on APL's ability to deliver domain export productivity 
      over problem exploration and solutions,
      but large data strains this productive workflow. 
      Domain experts increasingly require effective performance during 
      development stages while operating over large data, without 
      requiring reimplementation into other languages.
      Their workflow should not suffer because of the size of the data. 
    </para>
    <para>
      Unfortunately, interpreters underlie most APL implementations.
      APL interpreters exhibit impressive performance, in part because of the 
      excellent choice of abstractions <citation>kos</citation>, 
      but this approach scales poorly to 
      complicated computational hierarchies used today.
      Consider the following 
      computation, which comes from a Black Scholes formula for 
      predicting options pricing. <citation>blackscholes</citation>
    </para>
    <programlisting>D1←((⍟S÷X)+(r+(v*2)÷2)×T)÷vsqrtT</programlisting>
    <para>
      Dyalog APL's highly tuned primitives execute the above very quickly
      for small sizes. Unfortunately, 
      the interpreter must run these primitives in order; 
      as the array size grows, cache locality disintegrates, 
      and the interpreter lags behind the performance of hand-written C.
      Dyalog APL provides little support for GPU or distributed computing, 
      requiring too much programmer expertise to make either accessible to 
      the domain expert.
      The Co-dfns project addresses these issues through language 
      innovation and compiler based implementation.
    </para>
    <para>
      The Dyalog APL interpreter already uses threading to improve 
      performance of primitive operations. <citation>dyalogref</citation>
      Users have little control 
      over this behavior, and often, good parallelism 
      requires explicit consideration at the algorithmic level. Thus, 
      the Co-dfns language extends the traditional SIMD parallelism 
      and multi-threaded runtime features of APL with carefully designed 
      primitives for explicit task parallelism and synchronization. 
      This exposes explicit concurrency to users who can easily 
      write deterministic parallel programs. 
      Section 3 describes these language innovations in detail.
    </para>
    <para>
      The Co-dfns compiler uses a unique architecture, written in APL,
      which combines the nanopass approach 
      <citation>nanopass</citation> with the
      dfns language in single Dyalog APL namespace 
      as a functional composition of compiler passes.
      Though in the early stages, the
      implementation solves the issues of compiling APL
      using no garbage collector or closures, despite the
      higher-order style of APL programming. The compiler takes
      full advantage of existing LLVM optimizations
      and lays the groundwork for future
      high-level optimizations including function fusion and memory
      layout optimizations, two of the primary performance
      bottlenecks in APL. Users compile APL code
      interactively from within their APL sessions.
      The compiler reduces the need for users to rewrite
      hotspots in Cuda or C to scale effectively
      as data grows. As
      such, the design emphasizes scalable code.
      Section 4 describes the Co-dfns compiler.
      Though still immature as a compiler, some useful benchmarks and 
      performance results exist. Section 5 describes these. 
    </para>
  </section>
  <section>
    <title>Background</title>
    <para>
      APL first appeared in 1957 in <citetitle>A Programming Language</citetitle>
      by Kenneth Iverson <citation>apl</citation>, 
      who designed the language as an unambiguous 
      mathematical notation for teaching and research. This mathematical 
      and concise nature of the language make it attractive for certain 
      fields and experts. The first implementations of APL, however, 
      favored interactivity and dynamic behavior, using dynamic scoping 
      and mostly flat workspaces of functions. The use of goto statements, 
      execute functions, and dynamic scoping made it difficult to implement 
      full APL compilers. 
      In 1996, John Scholes introduced dfns, which provided a lexically scoped, 
      functionally oriented notation for writing APL. 
      <citation>dfns</citation> This notation 
      eliminates many barriers to compilation. See <xref linkend="syntax" /> 
      for a summary of the APL notation used in the following sections. 
      An operator is the APL term for a function which may accept functions 
      as arguments and returns a function. The results of an operator may be 
      bound to a name, used directly, or passed to other operators as arguments. 
      A namespace is the Dyalog term for a single module of APL code. 
      To create a namespace, a program fixes it from a namespace script, which 
      is the textual representation of the namespace stored as an 
      array or in a file.
    </para>
    <para>
      The LLVM project <citation>llvm</citation> provides a complete
      low-level compiler that transforms the LLVM intermediate
      representation (IR) into native code. It targets multiple
      architectures, including PTX (for Cuda programming), and x86_64.
      It provides many useful low-level optimizations that require
      significant effort to implement. This includes function inlining
      and loop vectorization. It also provides a MCJIT for 
      just-in-time compilation of code in-memory, which suits
      the needs of the Co-dfns compiler particularly well.
    </para>
    <para>
      When spawning a new parallel computation, in functional languages, 
      often the result of that computation goes into a future. <citation>futures</citation>
      The future 
      serves as a placeholder for the data until the parallel computation 
      completes. Attempting to read the future blocks until the data associated 
      with that future exists. An IVar, or immutable variable, holds at 
      most a single value in its lifetime. <citation>ivars</citation> When created, it holds no value, 
      and future computations may write at most once into that value. Attempting 
      to read the contents of the variable will block until some computation 
      writes to the variable. Futures and IVars often serve as the 
      synchronizing data structure in deterministic computation for functional 
      programming. <citation>singleassignment</citation>
    </para>
  </section>
  <table frame="void" id="syntax">
    <title>APL Syntax Summary</title>
    <thead>
      <tr>
        <th>Expression</th>
        <th colspan="2">Description</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td><literal>f A</literal></td>
        <td colspan="2">Apply <varname>f</varname> to <varname>A</varname>.</td>
      </tr>
      <tr>
        <td><literal>A f B</literal></td>
        <td colspan="2">
          Apply <varname>f</varname> to <varname>A</varname>
          and <varname>B</varname>.
        </td>
      </tr>
      <tr>
        <td><literal>V←Expr</literal></td>
        <td colspan="2">
          Give <literal>Expr</literal> the name <varname>V</varname>.
        </td>
      </tr>
      <tr>
        <td><literal>A[B]</literal></td>
        <td colspan="2">
          Extract array of same shape as <varname>B</varname> from 
          <varname>A</varname> by using each element in <varname>B</varname>
          as an index into <varname>A</varname>.
        </td>
      </tr>
      <tr>
        <td><literal>S1 ⋄ S2</literal></td>
        <td colspan="2">
          Evaluate <literal>S1</literal> and then <literal>S2</literal>.
        </td>
      </tr>
      <tr>
        <td><literal>{ ... }</literal></td>
        <td colspan="2">
          Function definition
        </td>
      </tr>
      <tr>
        <td><varname>⍺</varname></td>
        <td colspan="2">
          The left formal argument in a function definition.
        </td>
      </tr>
      <tr>
        <td><varname>⍵</varname></td>
        <td colspan="2">
          The right formal argument in a function definition.
        </td>
      </tr>
      <tr>
        <td><literal>T:E</literal></td>
        <td colspan="2">
          Inside a function, if <literal>T</literal> then return 
          <literal>E</literal>.
        </td>
      </tr>
      <tr>
        <td><literal>f¨A</literal></td>
        <td colspan="2">
          Apply <varname>f</varname> to each element in <varname>A</varname>.
        </td>
      </tr>
    </tbody>
  </table>
  <section>
    <title>Language Innovations</title>
    <para>
      The dfns language <citation>dfns</citation> 
      encourages spiritually functional
      data parallel programming; APL's bulk SIMD parallelism
      forms the language foundation, not a veneer over mostly 
      serial code. It does not, however, support a natural 
      expression of task parallelism. Normal APL operations map well 
      to vector engines, but not to OS threads and concurrent programming. 
    </para>
    <para>
      Traditional solutions to task parallelism in APL bolt on
      concurrency constructs such as mutexes and
      guards, with APL underneath. <citation>dyalogref</citation>
      This approach suffers from the
      same programmability problems that these same constructs have
      in any language, while additionally imposing a significant
      cost on language concision and productivity for
      domain experts. Such constructs serve domain experts little,
      even if the expert might otherwise benefit from a concurrent 
      paradigm. 
      Deterministic parallelism somewhat relieves these
      issues by eliminating by construction some undesirable 
      non-determinism in parallel programs. <citation>singleassignment</citation>
      The traditional expression 
      of these features does not map any more nicely to 
      APL than mutexes, guards, or the like.
    </para>
    <para>
      Task parallelism in APL ought to satisfy two criteria: 
      it should feel like APL, and it should be easy to use safely.
      That is, these constructs should not destroy
      APL's concise expression, nor greatly burden domain experts.
      Co-dfns extends the basic dfns language to enable
      both non-deterministic and deterministic task parallelism
      without requiring any additional syntax, and introducing only
      two new primitives to the language.
    </para>
    <para>
      Firstly, a new literal <literal>⌾</literal> (target) represents the
      new datatype, a single assignment array. A single assignment
      array works like other arrays except that a slot in the array
      receives at most one value in its lifetime. The <literal>⌾</literal>
      value behaves like the empty vector literal <literal>⍬</literal>,
      except that its fill value is a single assignment slot.
      The reshape <literal>S⍴⌾</literal> builds
      a single assignment array of shape <varname>S</varname>, where 
      <varname>S</varname> gives the dimensions of the array. This
      array begins empty, and each slot may be assigned a value at most once. 
    </para>
    <figure>
      <title>Single-assignment Array Example</title>
      <programlisting>A←2 2⍴⌾  ⍝ SA matrix of shape 2 2
A[0;0]←1 ⍝ Assign 0 0 slot to 1
A[0;0]←5 ⍝ Error, multiple writes</programlisting>
    </figure>
    <para>
      Attempting to read a slot in a single assignment array blocks until 
      that slot receives a value. In this way, single assignment arrays 
      deliver IVars in a fashion that completely integrates them with 
      normal APL: they feel like any other normal array during use. 
    </para>
    <para>
      Co-dfns provides concurrent execution through an operator
      <literal>∥</literal>. The expression <varname>F</varname><literal>∥</literal> is
      equivalent to the function <varname>F</varname> but when applied 
      returns immediately with a single assignment array whose values 
      will contain the results of executing <varname>F</varname> concurrently with the rest 
      of the program. Assuming <varname>F</varname> does not deadlock, 
      the array returned is equivalent to the array that would have been 
      returned by the function called without <literal>∥</literal> 
      assuming no error conditions.
    </para>
    <figure>
      <title>Example Naive Parallel Fibonacci</title>
      <programlisting>pfib←{
  1=⍵:1
  2=⍵:1
  (pfib<literal>∥</literal>⍵-1)+(pfib<literal>∥</literal>⍵-2)
}</programlisting>
    </figure>
    <para>
      These two primitives suffice to express task parallel computation 
      in Co-dfns. Synchronization occurs through single assignment arrays;
      concurrency arises through the parallel (<literal>∥</literal>) operator. 
      This approach requires no explicit control constructs or syntax.
      These primitives present an attractive alternative to
      traditional approaches because of the implicit and concise expression 
      of concurrency and synchronization. Moreover, by restricting all 
      inter-thread arrays (arrays touched by more than one thread) to single 
      assignment arrays, programs will exhibit determinism.
    </para>
  </section>
  <figure pgwide="0">
    <title>Co-dfns Compiler Architecture</title>
    <mediaobject>
      <imageobject>
        <imagedata align="center" width="3.25in" 
                   fileref="architecture.svg" format="SVG" />
      </imageobject>
    </mediaobject>
  </figure>
  <section>
    <title>Implementation</title>
    <para>
      The language of Co-dfns explicitly makes concurrency a tool 
      of thought for domain expert programming. The implementation of 
      Co-dfns also has explicit goals:
    </para>
    <orderedlist spacing="compact">
      <listitem>
        <para>
          Enable APL programs to scale to larger problems and modern 
          hardware, including massively parallel systems.
        </para>
      </listitem>
      <listitem>
        <para>
          Make it easy to integrate the compiler into existing APL 
          workflows.
        </para>
      </listitem>
      <listitem>
        <para>
          Support integration with external tools through well documented 
          interfaces and easy to use entry and exit points.
        </para>
      </listitem>
      <listitem>
        <para>
          Support compiled APL that seamlessly integrates with external 
          software products and systems.
        </para>
      </listitem>
    </orderedlist>
    <para>
      At this early stage, the compiler does not realize 
      all of these goals. Nonetheless, the current version does 
      solve all major obstacles to APL compilation, including the handling 
      of operators, nested functions, and possible ambiguity in parsing 
      APL due to the name class of certain variables (that is, whether a variable 
      is bound to a function or an array) in a scalable, performant manner.
    </para>
    <formalpara>
      <title>Architecture</title>
      <para>
        The Co-dfns compiler is itself implemented using dfns in Dyalog APL. 
        This makes the compiler easy to deploy and use. The compiler operates over 
        namespace scripts to create a Dyalog namespace, replacing
        the <function>⎕FIX</function> function in the interpreter. 
        <citation>dyalogref</citation>
        Users who wish to compile their code rather than 
        interpret it need only encapsulate their code into a single namespace, 
        and then fix it using the <function>CoDfns.Fix</function> function 
        instead of the <function>⎕FIX</function> function. They can optionally 
        produce an external LL IR file, which is a Clang and LLVM compatible 
        object to integrate with other LLVM based compilation objects. 
        <citation>llvm</citation>
      </para>
    </formalpara>
    <para>
      The compiler uses a pure-functional, nanopass structure, which divides 
      the compiler into a series of small passes which compose one 
      after another, each one transforming the incoming AST in some specific 
      way. The <function>GenLLVM</function> pass then takes the final AST 
      and converts it into an LLVM module for JITing or exporting to a file. 
    </para>
    <para>
      Being written in dfns, the compiler operates over a linearized XML
      tree that represents the AST, so that each compiler pass consumes
      and produces a valid XML tree that any user can serialize into
      standard format using the <function>⎕XML</function> function,
      <citation>quadxml</citation>
      easily inserting their own passes or changing the entry or exit
      points of the compiler for various reasons. In addition to being
      functional, which simplifies the design of the compiler, each pass
      tries to emphasize good APL style, focusing on aggregate, data
      parallel operations and minimizing recursion or complexity.
      As such, the compiler passes use no explicit loops or
      iterations, and they use little to no explicit recursion.
      The degree to which these passes can be expressed using minimal
      explicit control flow suggests that the Co-dfns compiler could
      serve as an excellent benchmark for executing compilers on the GPU.
      The compiler as a dfns benchmark also suggests places where memory
      handling could improve significantly in Dyalog APL
      and Co-dfns to enable a more functional style of APL program to
      perform well.
    </para>
    <para>
      Finally, the compiler demonstrates the flexibility and generality of the 
      Co-dfns language, in that the compiler itself finds a natural expression 
      therein. It shows how to create small, parallel friendly compiler passes 
      that exhibit none of the traditional complexity in compiler design. APL proves to 
      have a well designed set of primitives that encourages a concise and elegant 
      expression of the tree transformations required to implement a nanopass 
      compiler. This emphasis on aggregate, global operations over trees distinguishes 
      the approach used in Co-dfns from a regular nanopass compiler, 
      which tends to define each pass based on pattern matching and implicit 
      recursion. 
    </para>
    <formalpara>
      <title>Memory management</title>
      <para>
        Systems like Dyalog APL use a garbage collector to manage memory. 
        <citation>gc</citation>
        Garbage collectors liberate programming implementations and languages 
        greatly, and serve as the de facto standard memory management model 
        for most new languages. 
        <citation>gc2</citation>
        This is good. In Co-dfns, however, 
        because the language design enforces a natural stack lifetime on 
        variables and functions, the compiler uses a stack discipline to 
        implement all memory management, completely avoiding the extra 
        complexity and cost of integrating a high-performance garbage collector 
        into the runtime. 
      </para>
    </formalpara>
    <para>
      Interestingly, this decision does not preclude the use of operators, 
      which are higher-order functions in APL which return functions. Functions 
      cannot escape out of a lexical context, which would require indefinite 
      lifetimes of function values. These stack functions permit a wide range of 
      higher-order programming but require no closure allocation. Indeed, all 
      Co-dfns functions map directly to an LLVM function. Thus, the compiler 
      takes advantage of LLVM optimizations which might otherwise prove more 
      difficult or costly to implement directly, such as inlining. 
    </para>
    <formalpara>
      <title>Fusion</title>
      <para>
        Existing research into parallel system demonstrates the importance and 
        effectiveness of good fusion over certain operators. <citation>fusion</citation>
        The Dyalog interpreter 
        performance suffers even in cases where it might do well, because it 
        cannot fuse scalar primitive functions together to leverage modern 
        memory hierarchies. Full scalar function fusion does not yet exist in 
        the Co-dfns compiler, but the use of stack functions above enables the 
        inlining of many other explicit functions, including primitives. Further 
        increments of the compiler development will introduce scalar function 
        fusion.
      </para>
    </formalpara>
  </section>
  <figure pgwide="1">
    <title>Black Scholes Benchmark Results</title>
    <inlinemediaobject>
      <imageobject>
        <imagedata align="center" width="3.5in" 
                   fileref="linear.svg" format="SVG" />
      </imageobject>
    </inlinemediaobject>
    <inlinemediaobject>
      <imageobject>
        <imagedata align="center" width="3.5in"
                   fileref="logarithmic.svg" format="SVG" />
      </imageobject>
    </inlinemediaobject>
  </figure>
  <section>
    <title>Evaluation</title>
    <para>
      The Co-dfns compiler does not yet support the entire language, and it 
      does not currently implement many of the intended optimizations designed 
      for it. This makes evaluation challenging, as most benchmarks leverage 
      a fairly wide array of language features and primitives. Notwithstanding
      these challenges, the Co-dfns compiler can compile the Espen Haug 
      Black Scholes calculation. <citation>blackscholes</citation>
    </para>
    <formalpara>
      <title>Black Scholes benchmark</title>
      <para>
        The Black Scholes formula estimates options pricing, and the 
        implementation of Black Scholes favors APL interpreters to a fair 
        degree. Particularly, it requires very little code, and an APL 
        implementation contains very little structural control flow, 
        reducing the interpreter overhead. Moreover, the benchmark deals 
        mostly with scalar calculations over large arrays, for which 
        APL can implement specialized primitives that perform very well.
      </para>
    </formalpara>
    <para>
      The benchmark does a good job of measuring the performance of APL 
      on number crunching applications heavy with scalar computation. It 
      also benefits from good memory and cache locality as the size of 
      the data set increases. 
    </para>
    <formalpara>
      <title>Benchmarking Methodology</title>
      <para>
        To compare the Co-dfns compiler and Dyalog APL, the benchmarking 
        harness simulates as much as possible the intended use case of 
        the compiler. That is, execution of the compiled code should 
        occur as transparently as possible from within the Dyalog APL 
        system and should not require excessive tooling. 
      </para>
    </formalpara>
    <para>
      The current state of the compiler limits the level 
      of possible integration. In particular, the benchmark was 
      compiled offline and linked together as a shared object, rather 
      than taking advantage of LLVM's MCJIT system.
      <citation>llvm</citation>
      To use the shared 
      object from within the Dyalog interpreter, to properly simulate 
      the JIT behavior, a wrapper function written in the interpreter 
      converts the array data structures from that used by the interpreter 
      to that used by the compiled code and back once the compiled 
      code returns a result. The JIT implementation will remove the need 
      for such explicit wrapper functions.
      The compiled objects were compiled using Clang 3.4 with O3 optimizations
      but no other explicit optimization settings. 
    </para>
    <para>
      Each version of the benchmark, one for the interpreter and one for the 
      compiler, used the same APL implementation of the Black Scholes, though 
      the code used for the compiler was cosmetically tweaked to work around 
      some limitations in the current implementation. 
      Each version ran on each data set size three times, taking the average 
      of these three computations. Every run manually executed 
      the garbage collector before executing the code.
    </para>
    <para>
      The testing machine used an Intel Core i7-3610QM CPU @ 2.30Ghz with 
      16GB of RAM with Dyalog APL 14.0 Beta 3 on Red Hat Enterprise Linux 
      7.0 Beta. 
    </para>
    <formalpara>
      <title>Results</title>
      <para>
        See the figures for two versions of the same results, one on a 
        logarithmic scale and the other without any logarithmic adjustment. 
        The logarithmic scale demonstrates the performance characteristics 
        of the smaller sizes, while the linear scale demonstrates the 
        proportions of performance factors at large sizes better. 
      </para>
    </formalpara>
    <para>
      Note that the interpreter outperforms the naive, partial implementation 
      of the compiler at low data sizes, but then begins to suffer performance 
      degradation as the size increases. The compiler begins to outperform 
      the interpreter, increasingly so as the size increases. 
      The compiler outperforms the interpreter by 26% at the largest data set, 
      without performing any specific optimizations available to it. 
    </para>
    <para>
      The performance of the compiler at the low-end likely indicates the 
      inefficiency of the current approach to injecting and projecting data 
      to and from the compiled code, as this would most significantly influence 
      the results at the low-end. Moreover, the interpreter's handling of locality 
      likely contributes to its failure to scale linearly. 
      The compiler does a better job of this right now, but could undoubtedly 
      improve a great deal through the use of fusion optimizations. 
    </para>
  </section>
  <figure>
    <title>Black Scholes Benchmark Code</title>
    <programlisting>r←0.02 ⋄ v←0.03 ⋄ p←÷(○2)*0.5

CNDP←{
  K←÷1+0.2316419×L←|⍵
  R←p×(*(L×L)÷¯2)×{coeff+.×⍵*1+⍳5}¨K
  (1 ¯1)[B]×((0 ¯1)[B←⍵≥0])+R
}

BlackScholes←{
  expRT←*(-r)×T ⋄ vsqrtT←v×T*0.5
  D1←((⍟S÷X)+(r+(v*2)÷2)×T)÷vsqrtT
  D2←D1-vsqrtT
  CD1←CNDP D1 ⋄ CD2←CNDP D2
  R←(S×CD1)-X×expRT×CD2
  R,[0.5]((X×expRT×1-CD2)-S×1-CD1)
}</programlisting>
  </figure>
  <section>
    <title>Related Work</title>
    <para>
      Timothy Budd implemented an APL compiler for a traditional APL system.
      <citation>budd</citation>
      The work presents
      a unique and interesting approach to reducing computation on arrays
      by lazy execution. Budd's compiler delayed computation
      until the point at which the program requested the value. As in most
      traditional APL compilers, the accepted language does not coincide
      with normal APL programs of the time. Budd's
      compiler also managed allocation without requiring a garbage collector.
    </para>
    <para>
      Bob Bernecky implemented the APEX compiler <citation>apex</citation>.
      It has useful passes, such as data flow
      analysis and static single assignment. Moreover, Bernecky details an
      approach at introducing parallelism into the compiler itself. While 
      APEX is not self-hosting, its efforts to introduce parallel
      compiler passes should inform Co-dfns pass design.
      APEX does suffer from strong
      restrictions, though some could evaporate given enough
      effort. Both APEX and Co-dfns share some
      restrictions, such as not allowing the dynamic fixing of new functions
      at runtime. However, Co-dfns strives to ensure greater compatibility with 
      dfns programs in Dyalog than analogous programs and APEX.
    </para>
    <para>
      Dyalog has provided a good deal of input into Co-dfns, and
      so it makes sense that the upcoming release of their interpreter begins
      to implement some of the ideas in Co-dfns. This
      includes a facility for doing coarse-grained parallelism with futures,
      but does not include any ability to do more refined concurrent operations
      since the interpreter lacks single assignment arrays for synchronization;
      the interpreter itself makes no guarantees when effects occur in parallel.
      <citation>dyalogv14</citation>
    </para>
    <para>
      Single-assignment C <citation>sac</citation> 
      attempts to deliver a high-level, C-like language 
      that uses arrays as first-class data types. It focuses heavily on a 
      functional paradigm and automatically parallelizes code. The same 
      issues of memory copying and array management occur in SAC as in 
      Co-dfns, but SAC is a purely functional language, whereas Co-dfns 
      admits array mutation and variable assignment within a single scope. 
    </para>
    <para>
      The McLAB project <citation>mclab</citation>
      implements a compiler and supporting systems for MATLAB 
      code. Like Co-dfns, it provides an explicit intermediate language for 
      work, but goes to a lower level with its own JIT. It also produces 
      Fortran and C code, which are not explicit targets of Co-dfns. 
      The MATLAB language itself differs from Co-dfns, which naturally results
      in differences of approach with McLAB and Co-dfns. Co-dfns adapts the APL 
      language with explicit parallelism constructs, rather than emphasizing 
      automatic parallelization of the runtime primitives. 
    </para>
    <para>
      Languages like X10 <citation>x10</citation>, Fortress
      <citation>fortress</citation>, and Chapel <citation>chapel</citation>
      also strive to scale 
      programming to large distributed systems. They inherit much of 
      their linguistic history from Fortran, Java, and C++, rather than 
      APL. They also emphasize a more object-oriented approach than the 
      array-centric, functional approach of Co-dfns. They expose much 
      more explicit syntactic constructs for controlling data layout and 
      synchronization, whereas Co-dfns tries to minimize explicit syntax as 
      much as possible. 
    </para>
    <para>
      A number of systems such as ZPL <citation>zpl</citation>
      and Accelerate <citation>accelerate</citation> are able to
      provide interesting implementation strategies for array programming,
      each emphasizing different elements. They all take the overall approach
      of altering the language design in favor of making certain features
      prevalent. Accelerate, for instance, lifts rank to the type level,
      meaning that shapes are no longer first class entities. ZPL uses a
      more traditional language but enables predictable data layout for distributed
      computing. Accelerate takes advantage of the language and implementation to
      make heavy use of fusion.
    </para>
    <para>
      Eric Holk's Harlan system <citation>harlan</citation> 
      takes a unique approach. Targeting the GPU
      explicitly, it tries to introduce traditional programming concepts as
      native concepts on the GPU, so that traditional programs can run reasonably
      on the GPU. This approach, instead of lifting array programming to the
      general-purpose sphere, pulls general-purpose language constructs such as
      ADTs into the GPU and array world through the use of region inference and
      a number of other transformations. It also uses the nanopass style of
      compiler design and includes a macro system on top of it to reduce the
      number of core forms the compiler must consider.
    </para>
  </section>
  <section>
    <title>Conclusion</title>
    <para>
      Much work still remains to bring the compiler to its first public version. 
      This includes implementing more optimizations centered around 
      memory management, fusion, and targeting the GPU as well as the CPU. 
      Eventually, the compiler should also host itself, as it is written in 
      dfns. Among the optimizations that have the highest priority are the 
      work of Kai Trojahner <citation>implicit</citation>
      to reduce copying of arrays, minimizing allocations and maximizing 
      reuse of memory, and integrating richer and more native scalar function 
      fusion into the compiler. 
    </para>
    <para>
      Further work continues on exploring irregular problem domains, such as
      the compiler itself, and graph algorithms, such as the Graph500 Benchmark. 
      <citation>graph500</citation>
      Getting good results on these problems requires improved 
      memory management, such as recognizing shape and types of the array 
      values and doing preallocation of the memory regions before computation. 
      On benchmarks like the Graph500 this can save copying overhead in
      tight loops and improve GPU performance.
    </para>
    <para>
      The Co-dfns compiler, despite the early stages of its development, shows 
      promise for bringing robust scalability to the venerable APL language 
      in the context of modern hardware and platforms, on increasingly large 
      data sets. Unlike many efforts in the array field, the Co-dfns compiler 
      intends from the beginning to be accessible to the domain expert, 
      rather than targeting the computer scientist, and to exhibit suitable 
      industrial strengths for mature applications. The current stage of 
      development can only say so much regarding the future capacity of the 
      compiler, but it does demonstrate the feasibility of the approach and 
      a reasonable expectation of suitable performance gains.
    </para>
  </section>
  
  <section>
    <title>Acknowledgments</title>
    <para>
      Bob Bernecky provided excellent insight into the state of APL
      compilation. Dyalog, Ltd. has provided gracious funding for the
      development of this compiler. Edward Amsden, 
      Paul Ojanen, and others provided critical feedback. The 
      anonymous reviewers provided valuable insights.
    </para>
  </section>
  
  <bibliography>
    <title>References</title>
    <bibliomixed>
      <abbrev>ivars</abbrev>
      <personname>R.S. Nikhil Arvind</personname> and <personname>K. K. Pingali</personname>.
      <date>1989</date>.
      <title>I-structures: data structures for parallel computing</title>.
      In <citetitle>ACM Trans. Program. Lang. Syst.</citetitle>,
      <date>October 1989</date>.
    </bibliomixed>
    <bibliomixed>
      <abbrev>gc2</abbrev>
      <personname>Henry C. Baker, Jr.</personname> and <personname>Carl Hewitt</personname>. 
      <date>1977</date>. 
      <title>The incremental garbage collection of processes</title>. 
      In <citetitle>Proceedings of the 1977 symposium on Artificial intelligence and programming languages</citetitle>. 
      <publishername>ACM</publishername>, 
      <address>New York, NY, USA</address>, <pagenums>55-59</pagenums>. 
      DOI: <biblioid class="doi">http://doi.acm.org/10.1145/800228.806932</biblioid>
    </bibliomixed>
    <bibliomixed>
      <abbrev>apex</abbrev>
      <personname>Robert Bernecky</personname>. 
      <date>1997</date>.
      <citetitle>An Overview of the APEX Compiler</citetitle>.
      Technical Report 305/97. 
      <publishername>University of Toronto</publishername>,
      <address>Toronto, CA</address>. 
    </bibliomixed>
    <bibliomixed>
      <abbrev>budd</abbrev>
      <personname>Timothy Budd</personname>.
      <date>1988</date>.
      <citetitle>An APL Compiler</citetitle>.
      (<date>1 March 1988</date>).
      <publishername>Springer</publishername>
    </bibliomixed>
    <bibliomixed>
      <abbrev>mclab</abbrev>
      <personname>Andrew Casey</personname>, 
      <personname>Jun Li</personname>, 
      <personname>Jesse Doherty</personname>,
      <personname>Maxime Chevalier-Boisvert</personname>, 
      <personname>Toheed Aslam</personname>, 
      <personname>Anton Dubrau</personname>,
      <personname>Nurudeen Lameed</personname>, 
      <personname>Amina Aslam</personname>, 
      <personname>Rahul Garg</personname>,
      <personname>Soroush Radpour</personname>, 
      <personname>Olivier Savary Belanger</personname>, 
      <personname>Laurie Hendren</personname>, and
      <personname>Clark Verbrugge</personname>.
      <date>2010</date>.
      <title>McLab: An extensible compiler toolkit for MATLAB and related languages</title>.
      In <citetitle>C<superscript>3</superscript>S<superscript>2</superscript>E-10</citetitle>.
      <address>Montreal</address>.
    </bibliomixed>
    <bibliomixed>
      <abbrev>accelerate</abbrev>
      <personname>Manuel M.T. Chakravarty</personname>, 
      <personname>Gabriele Keller</personname>, 
      <personname>Sean Lee</personname>, 
      <personname>Trevor L. McDonell</personname>, and 
      <personname>Vinod Grover</personname>. 
      <date>2011</date>. 
      <title>Accelerating Haskell array codes with multicore GPUs</title>. 
      In <citetitle>Proceedings of the sixth workshop on Declarative aspects of multicore programming (DAMP '11)</citetitle>. 
      <publishername>ACM</publishername>, 
      <address>New York, NY, USA</address>, 
      <pagenums>3-14</pagenums>. 
      DOI: <biblioid class="doi">http://doi.acm.org/10.1145/1926354.1926358</biblioid> 
    </bibliomixed>
    <bibliomixed>
      <abbrev>chapel</abbrev>
      <personname>B.L. Chamberlain</personname>,
      <personname>D. Callahan</personname>, and
      <personname>H.P. Zima</personname>.
      <date>2007</date>.
      <title>Parallel Programmability and the Chapel Language</title>.
      In <citetitle>International Journal of High Performance Computing Applications  vol. 21 no. 3</citetitle>.
      (<date>August 2007</date>). <pagenums>291-312</pagenums>. 
    </bibliomixed>
    <bibliomixed>
      <abbrev>x10</abbrev>
      <personname>Philippe Charles</personname>, 
      <personname>Christian Grothoff</personname>,
      <personname>Vijay Saraswat</personname>, 
      <personname>Christopher Donawa</personname>, 
      <personname>Allan Kielstra</personname>, 
      <personname>Kemal Ebcioglu</personname>, 
      <personname>Christoph von Praun</personname>, and 
      <personname>Vivek Sarkar</personname>. 
      <date>2005</date>. 
      <title>X10: an object-oriented approach to non-uniform cluster computing</title>. 
      In <citetitle>Proceedings of the 20th annual ACM SIGPLAN conference on Object-oriented 
      programming, systems, languages, and applications (OOPSLA '05)</citetitle>. 
      <publishername>ACM</publishername>, 
      <address>New York, NY, USA</address>, <pagenums>519-538</pagenums>. 
      DOI: <biblioid class="doi">http://doi.acm.org/10.1145/1094811.1094852</biblioid>
    </bibliomixed>
    <bibliomixed>
      <abbrev>dyalogref</abbrev>
      <orgname>Dyalog Limited</orgname>.
      <date>2014</date>.
      <citetitle>Dyalog Language Reference</citetitle>
      (<edition>Version 14.0</edition>).
      <publishername>Dyalog Ltd.</publishername>, 
      <address>Bramley, UK</address>.
    </bibliomixed>
    <bibliomixed>
      <abbrev>futures</abbrev>
      <personname>Dan P. Friedman</personname> and <personname>David S. Wise</personname>.
      <date>1976</date>.
      <title>The Impact of Applicative Programming on Multiprocessing</title>.
      In <citetitle>International Conference on Parallel Processing</citetitle>, 
      <pagenums>263-272</pagenums>. 
    </bibliomixed>
    <bibliomixed>
      <abbrev>graph500</abbrev>
      <orgname>Graph500 Steering Committee</orgname>.
      <date>2010</date>.
      <title>Graph500 Benchmark 1</title>.
      (<date>October 2012</date>).
      Retrieved 11 April, 2014 from http://www.graph500.org/specifications
    </bibliomixed>
    <bibliomixed>
      <abbrev>sac</abbrev>
      <personname>Clemens Grelck</personname> and <personname>Sven-Bodo Scholz</personname>.
      <date>2006</date>.
      <title>SAC — A FunctionalArray Language for Efficient Multi-threaded Execution</title>.
      In <citetitle>International Journal of Parallel Programming, Vol. 34, No. 4, (August 2006)</citetitle>.
      DOI: <biblioid class="doi">0.1007/s10766-006-0018-x</biblioid>
    </bibliomixed>
    <bibliomixed>
      <abbrev>fusion</abbrev>
      <personname>Clemens Grelck</personname>,
      <personname>Karsten Hinckfuß</personname>, and
      <personname>Sven-Bodo Scholz</personname>. 
      <date>2006</date>.
      <title>With-Loop Fusion for Data Locality and Parallelism</title>.
      In <citetitle>Implementation and Application of Functional Languages
      Lecture Notes in Computer Science Volume 4015</citetitle>, 
      <date>2006</date>, <pagenums>178-195</pagenums>.
    </bibliomixed>
    <bibliomixed>
      <abbrev>cosmos</abbrev>
      <person>
        <personname>Paul Grosvenor</personname>
        <affiliation>Optima Systems</affiliation>
      </person>.
      <date>2013</date>.
      <title>COSMOS Performance Improvements</title>.
      Video. In <citetitle>Dyalog '13 Invited Guest and User Presentations</citetitle>.
      <date>(October 2013)</date>. 
      DOI: <biblioid class="doi">http://video.dyalog.com/Dyalog13/?v=oK_XGobiFmM</biblioid>
    </bibliomixed>
    <bibliomixed>
      <abbrev>harlan</abbrev>
      <personname>Eric Holk</personname>,
      <personname>William Byrd</personname>,
      <personname>Nilesh Mahajan</personname>,
      <personname>Jeremiah Willock</personname>,
      <personname>Arun Chauhan</personname>,
      <personname>Andrew Lumsdaine</personname>.
      <date>2011</date>.
      <title>Declarative Programming for GPUs</title>.
      In <citetitle>International Conference on Parallel Computing (ParCo 2011)</citetitle>.
      (<date>September 2011</date>).
    </bibliomixed>
    <bibliomixed>
      <abbrev>apl</abbrev>
      <personname>Kenneth Iverson</personname>.
      <date>1962</date>.
      <citetitle>A Programming Language</citetitle>
      (<edition>4th</edition> ed.).
      <publishername>Wiley</publishername>,
      <address>New York, NY</address>.
    </bibliomixed>
    <bibliomixed>
      <abbrev>nanopass</abbrev>
      <personname>Andrew W. Keep</personname> and <personname>R. Kent Dybvig</personname>. 
      <date>2013</date>. 
      <title>A nanopass framework for commercial compiler development</title>. 
      In <citetitle>Proceedings of the 18th ACM SIGPLAN international conference on Functional programming (ICFP '13)</citetitle>.
      <publishername>ACM</publishername>, <address>New York, NY, USA</address>, 
      <pagenums>343-350</pagenums>. 
      DOI: <biblioid class="doi">http://doi.acm.org/10.1145/2500365.2500618</biblioid>
    </bibliomixed>
    <bibliomixed>
      <abbrev>dyalogv14</abbrev>
      <personname>Morten Kromberg</personname> and <personname>Jay Foad</personname>.
      <date>2013</date>.
      <title>Parallel Language Features in Version 14.0</title>.
      Video. In <citetitle>Dyalog '13</citetitle>.
      DOI: <biblioid class="doi">http://video.dyalog.com/Dyalog13/?v=Bmx_yUKxVv0</biblioid>
    </bibliomixed>
    <bibliomixed>
      <abbrev>mortenemail</abbrev>
      <personname>Morten Kromberg</personname>.
      <date>2014</date>.
      <title>Re: The Market for APL</title>.
      Personal email conversation, <date>March 2014</date>.
    </bibliomixed>
    <bibliomixed>
      <abbrev>llvm</abbrev>
      <personname>Chris Lattner</personname>.
      <date>2011</date>.
      <title>LLVM and Clang: Advancing Compiler Technology</title>.
      In <citetitle>Free and open Source Developers' European Meeting (FOSDEM 2011)</citetitle>. 
      (<date>February 2011</date>). <address>Brussels, Belgium</address>.
    </bibliomixed>
    <bibliomixed>
      <abbrev>zpl</abbrev>
      <personname>Calvin Lin</personname> and <personname>Lawrence Snyder</personname>.
      <date>1994</date>.
      <title>ZPL: An array sublanguage</title>.
      In <citetitle> Languages and Compilers for Parallel Computing Lecture Notes in Computer Science Volume 768</citetitle>. 
      <pagenums>96-114</pagenums>.
    </bibliomixed>
    <bibliomixed>
      <abbrev>gc</abbrev>
      <personname>John McCarthy</personname>. 
      <date>1960</date>. 
      <title>Recursive functions of symbolic expressions and their computation by machine, Part I</title>. 
      In <citetitle>Commun. ACM 3, 4 (April 1960)</citetitle>, <pagenums>184-195</pagenums>. 
      DOI: <biblioid class="doi">http://doi.acm.org/10.1145/367177.367199</biblioid>
    </bibliomixed>
    <bibliomixed>
      <abbrev>dfns</abbrev>
      <personname>John Scholes</personname>.
      <date>2009</date>.
      <title>Introduction to D-functions</title>.
      Video.
      In <citetitle>Dyalog '13 Seminars</citetitle>.
      (<date>September 2009</date>).
      DOI: <biblioid class="doi">http://video.dyalog.com/Dyalog09/#INTRODUCTIONTOD-FUNCTIONS-PART1</biblioid>
    </bibliomixed>
    <bibliomixed>
      <abbrev>quadxml</abbrev>
      <personname>Richard Smith</personname>.
      <date>2009</date>.
      <title><literal>⎕XML</literal> and Journaling File System</title>.
      Video.
      In <citetitle>Dyalog '13 Seminars</citetitle>.
      (<date>September 2009</date>).
      DOI: <biblioid class="doi">http://video.dyalog.com/Dyalog09/RichardSmith_QuadXML.html</biblioid>
    </bibliomixed>
    <bibliomixed>
      <abbrev>fortress</abbrev>
      <personname>Guy L. Steele Jr</personname>. 
      <date>2006</date>.
      <title>Parallel Programming and Parallel Abstractions in Fortress</title>.
      In <citetitle>FLOPS</citetitle>.
    </bibliomixed>
    <bibliomixed>
      <abbrev>kos</abbrev>
      <personname>Stephen Taylor</personname>.
      <date>In press</date>.
      <title>Impending kOS</title>.
      In <citetitle>Vector: journal of the British APL Association</citetitle>.
      DOI: <biblioid class="doi">http://archive.vector.org.uk/art10501320</biblioid>
    </bibliomixed>
    <bibliomixed>
      <abbrev>singleassignment</abbrev>
      <personname>L. G. Tesler</personname> and <personname>H. J. Enea</personname>.
      <date>1968</date>. 
      <title>A language design for concurrent processes</title>. 
      In <citetitle>AFIPS</citetitle>,
      <date>Spring 1968</date>.
    </bibliomixed>
    <bibliomixed>
      <abbrev>implicit</abbrev>
      <personname>Kai Trojahner</personname>.
      <date>2005</date>.
      <title>Implicit Memory Management for a Functional Array Processing Language</title>.
      (<date>April 2005</date>).
    </bibliomixed>
    <bibliomixed>
      <abbrev>blackscholes</abbrev>
      <orgname>Wikipedia</orgname>. <date>2014</date>.
      <title>Black–Scholes model</title>.
      (<date>April 2014</date>).
      Retrieved <date>11 April, 2014</date> from
      https://en.wikipedia.org/wiki/Black%E2%80%93Scholes
    </bibliomixed>
  </bibliography>
</article>
