\documentclass[numbers,preprint]{sigplanconf}
% \usepackage[utf8]{inputenc}
\usepackage{array}
\usepackage{fontspec}
\setmainfont[Ligatures=TeX]{STIX}
\setmonofont{APL385 Unicode}
\usepackage[scaled]{helvet} % see www.ctan.org/get/macros/latex/required/psnfss/psnfss2e.pdf
\usepackage{url}                  % format URLs
\usepackage{listings}          % format code
\usepackage{enumitem}      % adjust spacing in enums
\usepackage[colorlinks=true,allcolors=blue,breaklinks,draft=false]{hyperref}   % hyperlinks, including DOIs and URLs in bibliography
% known bug: http://tex.stackexchange.com/questions/1522/pdfendlink-ended-up-in-different-nesting-level-than-pdfstartlink
\newcommand{\doi}[1]{doi:~\href{http://dx.doi.org/#1}{\Hurl{#1}}}   % print a hyperlinked DOI
\usepackage{booktabs}

\begin{document}

\title{The Key to a Data Parallel Compiler}

\conferenceinfo{ARRAY '16}{June, 2016, Santa Barbara, CA, USA}
\copyrightyear{2016}

\authorinfo{Aaron W. Hsu}{Indiana University}{awhsu@indiana.edu}

\maketitle
\begin{abstract}
We present a language-driven strategy for the construction of 
compilers that are inherently data-parallel in their design and 
implementation. Using an encoding of the inter-node relationships 
between nodes in an AST called a Node Coordinate Matrix, we demonstrate how 
an operator called the Key operator, that applies a function over groupings 
of array cells grouped and ordered by their keys, when used in conjunction 
with a Node Coordinate Matrix, permits arbitrary computation over sub-trees 
of an AST using purely data-parallel array programming techniques. We discuss 
the use of these techniques when applied to an experimental commercial 
compiler called Co-dfns, which is a fully data-parallel compiler developed 
using the techniques discussed here.
\end{abstract}

% Proposed new outline:
% 
% I. 	Generalization on manipulations over Sub-tree computation
%   A.	Key + Encoding → Data parallel, Array-based computation
%   B.	Easy to write/read
%   C.	Uniform across passes
% II.	Case Studies
%   A.	Function Lifting
%   B.	Expression Flattening
%   C.	Lexical Scope Resolution
%   D.	Illustrates the problem space
% III.	Other passes/Issues
%   A.	Discussion of other passes
%   B.	Beyond the Key Operator
% IV.	In practice/Co-dfns Compiler
% V.	Performance/Results
% 
% Questions that must be addressed:
% 
% 1. Why the formalization in APL?
% 2. Why parallelize a compiler?
% 3. How well does it work/scale/generalize

\section{Introduction}

Compilers represent a peculiar class of tree/graph algorithms that greatly 
alter the underlying structure of the input graph into something very 
different in structure, but equivalent by some measure of interpretation. 
These algorithms are widely applicable, but have stubbornly resisted a 
general approach to parallelization. Most compilers are single-threaded 
or make very limited use of parallelism in an ad hoc way. The design and 
analysis of compilers almost universally deals with compilers as recursive 
traversals over ASTs. Such formulations usually include heavy reliance on 
branching, recursion, and sometimes very sophisticated control flow in 
practice. All of this contributes to make it difficult to effectively 
parallelize such algorithms onto architectures that are sensitive to 
branching and recursion, such as GPGPUs or highly vectorized CPUs. 

There has been some success in efficiently executing parts of a compiler 
on the GPU, such as the parser \cite{bunda1984apl}, 
tokenization \cite{bernecky2003tokenizer}, and certain compiler 
analyses \cite{prabhu2011eigencfa,mendez2012inclusion}. However, a generalized 
framework or strategy for parallel compilation remains elusive. One of 
the major difficulties is the recursive and highly branch dependent nature 
of the algorithms. These present challenges that must be overcome in 
vector-centric architectures. 

We present a language-driven strategy for creating an inherently parallel 
compiler. We select a set of well known data-parallel primitives, and then 
restrict program construction to the composition of these operations into 
functions, without any forms of non-linear control flow, such as branching, 
recursion, or pattern matching. Programs constructed in such a style are 
data-parallel by construction. A key problem to the construction of compilers 
in such a restricted environment is dealing with the inherently recursive and 
nested structure of the AST. By working over a linearized representation of 
the AST as a matrix, combined with a structure we call a Node Coordinate 
Matrix, and using data-parallel primitives, we are able to tame the recursive 
and nested nature of the AST into something that can be processed handily 
using only data-parallel, data-flow programs, without requiring branching 
or other forms of non-data parallel control flow. 

In the following sections we describe the core methods and idioms surrounding 
the AST encoding, construction of the Node Coordinate Matrix, and the use 
of these structures in handling nested, recursive AST relationships. This 
enables us to perform arbitrary computation over arbitrary sub-trees of an 
AST selected by parent-child relationships in the manner often seen in 
compiler transformations. These ideas have been further implemented and tested 
through the implementation of a complete data-parallel by construction 
compiler, called Co-dfns, that compiles a lexically scoped, functionally 
oriented dialect of APL with nested functions. The compiler targets both the 
GPU and the CPU, and its core compiler is implemented in this pure, restricted 
language that uses only function composition over data-parallel primitives. 

\paragraph{Contributions}

\begin{itemize}[noitemsep]

\item We describe a method of computing over arbitrary sub-trees
selected by their parent-child relationships in a data-parallel manner
using the Key operator and Node Coordinates.

\item We situation this technique into a broader, language-driven 
strategy for compiler construction that enables the development of parallel 
by construction compilers that are high-level in their implementation and 
exceptionally concise.

\item We provide analysis of these techniques as used in a commercial
compiler project called Co-dfns and report on the results,
including the overall architecture of the compiler, the uses of these 
techniques within the compiler, and the demonstration of these techniques 
applied to two specific compiler passes.

\end{itemize}

\section{Notational Conventions}

For space and convenience, we use a concise array notational convention to 
describe our techniques and approach. The notation is executable and well 
established in the array community (it is a limited subset of APL). This is 
in fact the same notation used within the Co-dfns compiler itself, whose 
source code is available online and provides a complete example of the use 
of both these algorithms and this notation in the large. 

All code examples are given in the following form:

\begin{verbatim}
      5×3+4 ⍝ Right to Left Evaluation
35
\end{verbatim}

That is, the sample code given indented by 6 spaces, followed by the value 
of the expression without indent. All expressions are evaluated right to 
left and all function application is unified and infix; that is, a function 
application may apply a function (such as \verb;+;) to one or two arguments, 
which must appear on the right and (optionally) on the left of the function 
name, as in the above example. Tables \ref{tab:scalarprims} to 
\ref{tab:conjunctions}
provide a listing of all of the primitives used in the Co-dfns compiler, 
a selection of which are used here. 

We separate out higher-order functions called operators, from functions that 
operate over arrays that we call simply functions. Operators take a function 
argument or two, and bind more strongly to the left than the right. An 
operator may take one or two operands, either on the left, or on the 
right and left, and will return a function. In the following 
example, we apply a reduction operator to a function created using the 
composition operator, as an example:

\begin{verbatim}
      +∘-/1 2 3 4 5
3
      +/1 2 3 4 5
15
\end{verbatim}

Uses of the notation are introduced slowly throughout the treatment, so 
the reader is encouraged to simply proceed through the treatment, referencing 
the tables only when required. 

\begin{table}
\centering
\begin{tabular}{cll}
\toprule
Symbol                   & Monadic            & Dyadic \\
\midrule
\texttt{+}               & Identity           & Plus (Add) \\
\texttt{-}               & Negative           & Minus (Subtract) \\
\texttt{×}               & Direction (Signum) & Times (Multiply) \\
\texttt{÷}               & Reciprocal         & Divide \\
\texttt{|}               & Magnitude          & Residue (Modulo) \\
\texttt{⌊}               & Floor              & Minimum \\
\texttt{⌈}               & Ceiling            & Maximum \\
\texttt{*}               & Exponential        & Power \\
\texttt{⍟}               & Natural Logarithm  & Logarithm \\
\texttt{○}               & Pi Times           & Circular (Trigonometric) \\
\texttt{!}               & Factorial          & Binomial \\
\texttt{\textasciitilde} & Not                & \\
\texttt{?}               & Roll               & \\
\texttt{∧}              &                    & And \\
\texttt{∨}               &                    & Or \\
\texttt{⍲}               &                    & Nand \\
\texttt{⍱}               &                    & Nor \\
\texttt{<}               &                    & Less \\
\texttt{≤}               &                    & Less Or Equal \\
\texttt{=}               &                    & Equal \\
\texttt{≥}               &                    & Greater Or Equal \\
\texttt{>}               &                    & Greater \\
\texttt{≠}               &                    & Not Equal \\
\end{tabular}
\caption{Scalar Primitives}
\label{tab:scalarprims}
\end{table}

\begin{table}
\centering
\begin{tabular}{cll}
\toprule
Symbol     & Monadic       & Dyadic \\
\midrule
\texttt{⍴} & Reshape       & \\
\texttt{,} & Ravel         & Catenate/Laminate \\
\texttt{⍪} & Table         & Catenate First/Laminate \\
\texttt{⌽} & Reverse       & Rotate \\
\texttt{⊖} & Reverse First & Rotate First \\
\texttt{⍉} & Transpose     & Transpose \\
\texttt{↑} & Mix           & \\
\texttt{↓} & Split         & \\
\texttt{⊂} & Enclose       & Partitioned Enclose \\
\texttt{∊} & Enlist        & \\
\end{tabular}
\caption{Structural Mixed Primitives}
\label{tab:structprims}
\end{table}

\begin{table}
\centering
\begin{tabular}{cll}
\toprule
Symbol                   & Monadic  & Dyadic \\
\midrule
\texttt{⊃}               & Disclose & Pick\\
\texttt{↑}               &          & Take \\
\texttt{↓}               &          & Drop \\
\texttt{/}               &          & Replicate \\
\texttt{⌿}               &          & Replicate First \\
\texttt{\textbackslash}  &          & Expand \\
\texttt{⍀}               &          & Expand First \\
\texttt{\textasciitilde} &          & Without (Excluding) \\
\texttt{∩}               &          & Intersection \\
\texttt{∪}               & Unique   & Union \\
\texttt{⊣}               & Same     & Left \\
\texttt{⊢}               & Identity & Right\\
\end{tabular}
\caption{Selection Mixed Primitives}
\label{tab:selprims}
\end{table}

\begin{table}
\centering
\begin{tabular}{cll}
\toprule
Symbol     & Monadic         & Dyadic \\
\midrule
\texttt{⍳} & Index Generator & Index Of \\
\texttt{∊} &                 & Membership \\
\texttt{⍋} & Grade Up        & Grade Up \\
\texttt{⍒} & Grade Down      & Grade Down \\
\texttt{?} &                 & Deal \\
\texttt{⍷} &                 & Find \\
\end{tabular}
\caption{Selector Mixed Primitives}
\label{tab:seltorprims}
\end{table}

\begin{table}
\centering
\begin{tabular}{cll}
\toprule
Symbol     & Monadic       & Dyadic \\
\midrule
\texttt{⍴} & Shape         & \\
\texttt{≡} & Depth         & Match \\
\texttt{≢} & Tally         & Not Match \\
\texttt{⍎} & Execute       & Execute \\
\texttt{⍕} & Format        & Format \\
\texttt{⊥} &               & Decode (Base) \\
\texttt{⊤} &               & Encode (Representation) \\
\texttt{⌹} & Matrix Divide & Matrix Inverse \\
\end{tabular}
\caption{Miscellaneous Mixed Primitives}
\label{tab:miscprims}
\end{table}

\begin{table*}
\centering
\begin{tabular}{cll}
\toprule
Symbol     & Name & Description \\
\midrule
\texttt{⍨} & Commute & Swaps arguments or distributes
 right argument to both sides \\
\texttt{¨} & Each & Applies its operand point-wise over the left/right
 arguments \\
\texttt{/} & Reduce & Reduce along the last axis \\
\texttt{⌿} & Reduce First & Reduce along the first axis \\
\texttt{\textbackslash} & Scan & Scan along the last axis \\
\texttt{⍀} & Scan First & Scan along the first axis \\
\texttt{⌸} & Key & Apply operand once for each sub-array grouped by key \\
\end{tabular}
\caption{Primitive Monadic/Unary Operators, each takes a single left
operand and describes a function operating over one or two arguments}
\label{tab:adverbs}
\end{table*}

\begin{table*}
\centering
\begin{tabular}{cll}
\toprule
Symbol     & Name & Description \\
\midrule
\texttt{∘} & Compose & Composes two operands as in traditional mathematics \\
\texttt{.} & Inner Product & Inner product operation, e.g.
 \texttt{+.×} for matrix multiplication \\
\texttt{∘.} & Outer Product & Cartesian product or ``function table'' \\
\texttt{⍣} & Power & Iteration, Limited use only \\
\texttt{⍤} & Rank & Apply a function along cells of an array \\
\end{tabular}
\caption{Primitive Dyadic/Binary Operators, each takes a left
 and right operand and describes a function operating over one or two arguments}
\label{tab:conjunctions}
\end{table*}

\section{Data Parallel Sub-tree Computation}

Computing over sub-trees normally involves recursing down the structure of 
the AST, identifying the root nodes of each sub-tree, and then dispatching
to a handler for that sub-tree, which will continue the recursion at that 
point and return the modified tree, which will replace the previous 
sub-tree. More complicated transformations may involved moving the root 
nodes around in the tree or other large, distant structural modifications. 

We divide the work of sub-tree computation into three basic phases. Firstly, 
we maintain a node coordinate with each node that uniquely identifies it 
relative to others, we use these coordinates to select and group nodes for 
work as a single sub-tree, and then we operate over these groupings using 
the key operator.

\subsection{Encoding the AST}

We represent the AST as a matrix of 3 columns and n rows, one for
each node in the tree. The first column contains the inter-node
relationships in the form of a depth vector. The second column is a
vector of the node types, while the third contains the ``value''
of the node, such as the name in a variable.

Parsing the examples above gives the following ASTs:

\begin{verbatim}
      F(f)              E(v)
       │         ┌───────┼──────┐
       E         E      P(÷)    E
     ┌─┴─┐  ┌────┼────┐    ┌────┼────┐
     F   A V(a) P(+) V(b) V(c) P(×) V(d)
     │   │
     E  N(7) 
 ┌───┼────┐
V(⍵) P(+) V(⍵)
\end{verbatim}

The depth vector for these trees we name \verb;Fd; and \verb;Ed;, respectively:

\begin{verbatim}
Fd←0 1 2 3 4 4 4 2 2 3
Ed←0 1 2 2 2 1 1 2 2 2
\end{verbatim}

The node types we call \verb;Ft; and \verb;Et;:

\begin{verbatim}
Ft←’FEFEVPVAN’
Et←’EEVPVPEVPV’
\end{verbatim}

And finally, we call the values vectors \verb;Fv; and \verb;Ev;:

\begin{verbatim}
Fv←’f’ 0 0 0 ‘⍵’ ‘+’ 0 7
Ev←’v’ 0 ‘a’ ‘+’ ‘b’ ‘÷’ 0 ‘c’ ‘×’ ‘d’
\end{verbatim}

We combine these to form the respective AST matrices. We write \verb;A,B;
to catenate arrays \verb;A; and \verb;B; along their last axes and \verb;⍪A; to turn a
vector into a 1-column matrix. Thus, the two tree matrices are given
by the following expressions:

\begin{verbatim}
      Fd,Ft,⍪Fv
0 F f
1 E 0
2 F 0
3 E 0
4 V ⍵
4 P +
4 V ⍵
2 A 0
3 N 7
      Ed,Et,⍪Ev
0 E v
1 E 0
2 V a
2 P +
2 V b
1 P ÷
1 E 0
2 V c
2 P ×
2 V d
\end{verbatim}

Note especially that all inter-node information lives in the depth
vector, but that this information requires non-local access as
described in the previous section to use it. Constructing node
coordinates fixes this issue.

\subsection{Node Coordinates}

Every node in an AST has a node coordinate, named because a coordinate
is a precise location in a space.  We can imagine all the nodes
arranged inside some multi-dimensional space, leading to a specific
set of node coordinates. Many such arrangements exist, of varying
usefulness. In our case, any arrangement should allow us to answer
the following questions about any two arbitrary nodes in a tree:

\begin{enumerate}[noitemsep]
\item Are the nodes the same?
\item Are they siblings?
\item Does one appear ``earlier'' in the tree?
\item Are they at the same depth?
\item Is one an ancestor of another?
\end{enumerate}

In short, we care about the relative position of each node in a tree
relative to any other. We define a node coordinate as follows.

A node coordinate is a vector whose length is the depth of the
tree. Its elements are natural numbers.  The count of non-zero elements
in the vector is equal to the depth of that node in the tree. All zero
elements appear after the non-zero elements. That is, a coordinate
is zero-padded on the right. When ordered lexicographically, the
nodes for each coordinate appear in order according to a depth-first
pre-order traversal of the tree. Each coordinate uniquely identifies a
single node. Every ancestor’s coordinate is a prefix of any child’s
coordinate, ignoring zeros. From the above it follows that every node
is lexicographically greater than its left sibling and differs from
it by exactly one non-zero element, and that this element is the last
non-zero element in the coordinate.

To understand how we construct these coordinates we must consider
how we represent the AST.

\subsubsection{Constructing Node Coordinates}

In building the node coordinates, our goal is to build a matrix where
each row is a node coordinate satisfying our previously described
requirements. We end up with an n d shaped matrix where n is the number
of rows and d the depth of the tree. As noted above, the depth vector
contains all the necessary information, but in the wrong form.

We write \verb;f⌿A; to reduce the first axis of \verb;A; using function \verb;f;. Thus,
\verb;+⌿V; is the sum of the elements in vector \verb;V;. The function \verb;x⌈y;
gives the maximum of its two arguments. So, the depth of each tree
is given by the following:

\begin{verbatim}
      1+⌈⌿Fd
5
      1+⌈⌿Ed
3
\end{verbatim}

We can obtain the ordered sequence  by writing \verb;⍳n;:

\begin{verbatim}
      ⍳3
0 1 2
\end{verbatim}

So the depths of all nodes that appear in the depth vectors is thus:

\begin{verbatim}
      ⍳1+⌈⌿Fd
0 1 2 3 4 5
      ⍳1+⌈⌿Ed
0 1 2
\end{verbatim}

The function table or outer product of \verb;f; over vectors \verb;U; and \verb;V; is
written \verb;U ∘.f V; giving a \verb;U V; shaped matrix as a result. Thus,
\verb;(⍳3)∘.×⍳3; gives a small multiplication table:

\begin{verbatim}
      (⍳3)∘.×⍳3
0 0 0
0 1 2
0 2 4
\end{verbatim}

If we use \verb;=; instead, we have a Boolean identity matrix:

\begin{verbatim}
      (⍳3)∘.=⍳3
1 0 0
0 1 0
0 0 1
\end{verbatim}

If we use \verb;∘.=; on the depth vector and its set of depths instead,
we see an expanded Boolean representation of the depth vector:

\begin{verbatim}
      Fd∘.=⍳1+⌈⌿Fd
1 0 0 0 0
0 1 0 0 0
0 0 1 0 0
0 0 0 1 0
0 0 0 0 1
0 0 0 0 1
0 0 0 0 1
0 0 1 0 0
0 0 0 1 0
      Ed∘.=⍳1+⌈⌿Ed
1 0 0
0 1 0
0 0 1
0 0 1
0 0 1
0 1 0
0 1 0
0 0 1
0 0 1
0 0 1
\end{verbatim}

These matrices let us see the nesting features of each tree more
visually, but also suggest another step. We can compute a sum scan
with \verb;+⍀;, also called a prefix sum, along the first axis. Applying
this function on the above matrices leads to an interesting result:

\begin{verbatim}
      +⍀Fd∘.=⍳1+⌈⌿Fd
1 0 0 0 0
1 1 0 0 0
1 1 1 0 0
1 1 1 1 0
1 1 1 1 1
1 1 1 1 2
1 1 1 1 3
1 1 2 1 3
1 1 2 2 3
      +⍀Ed∘.=⍳1+⌈⌿Ed
1 0 0
1 1 0
1 1 1
1 1 2
1 1 3
1 2 3
1 3 3
1 3 4
1 3 5
1 3 6
\end{verbatim}

These matrices are lexicographically ordered, and each ancestor
shares a common prefix with its descendants. They are also unique
coordinates. Only the spurious digits at the end of each coordinate
prevent these matrices from meeting all our requirements for valid
node coordinates.

The expression \verb;V f⍤¯1⊢M; applies \verb;f; to corresponding elements of
\verb;V; and rows of \verb;M;:

\begin{verbatim}
      3 3⍴⍳9
0 1 2
3 4 5
6 7 8
      (⍳3)+⍤¯1⊢3 3⍴⍳9
0 1  2
4 5  6
8 9 10
\end{verbatim}

If \verb;n↑V; takes the first \verb;n; elements of \verb;V;, then we can obtain coordinate
matrices from the prefix sums by noting that the spurious digits
all come after column \verb;d+1; where \verb;d; is the depth corresponding to that
coordinate. The following gives a complete expression for computing
a node coordinate matrix from a depth vector, shown using \verb;Fd; and \verb;Fe;:

\begin{verbatim}
      ⊢Fc←(1+Fd)↑⍤¯1⊢+⍀Fd∘.=⍳1+⌈⌿Fd
1 0 0 0 0
1 1 0 0 0
1 1 1 0 0
1 1 1 1 0
1 1 1 1 1
1 1 1 1 2
1 1 1 1 3
1 1 2 0 0
1 1 2 2 0
      ⊢Ec←(1+Ed)↑⍤¯1⊢+⍀Ed∘.=⍳1+⌈⌿Ed
1 0 0
1 1 0
1 1 1
1 1 2
1 1 3
1 2 0
1 3 0
1 3 4
1 3 5
1 3 6
\end{verbatim}

A careful study of the definition of a node coordinate and the
above construction should reveal why this works. Intuitively, we are
creating a multi-dimensional space or a number system in which each
digit place or dimension contains or circumscribes a smaller space
in which are contained all the descendant nodes that appear lower in
the tree. Each coordinate is a sort of special path through the tree
encoded to have desirable properties relative to other paths.

\subsubsection{Operations on Node Coordinates}

The simplest operation over a node coordinate is to extract the depth
of the node. The expression \verb;C⍳0; finds the first occurrence of \verb;0; in \verb;C;
and returns the index of that occurrence. (The application of ⍳ here
is its dyadic form, meaning that it receives two arguments. Previous
uses of ⍳ have used its monadic form, which generates all indices,
instead of finding a specific index. Most APL functions have a
dyadic and monadic form.)  Thus, the depth of a node is given by
the following:

\begin{verbatim}
      C←1 1 2 0 0
      ¯1+C⍳0
2
\end{verbatim}

The primary calculation we care about in order to do function lifting,
expression flattening, and other sorts of flattening transformations
is to determine which nodes are ancestors of another node. The core
of this computation determines whether one node is a child of another
based on their node coordinates. This amounts to computing whether
one node is a prefix of another, ignoring zeros.

We write \verb;(f g h); to represent the composition of functions \verb;f;, \verb;g;,
and \verb;h; as a function train. This is used with another shorthand of
composition according to the following equivalences:

\begin{verbatim}
A(f g h)B ←→ (A f B) g (A h B)
A(0 f h)B ←→ 0 f (A h B) ⍝ Constant Case
A(f g)B   ←→ f (A g B)
\end{verbatim}

With this, we can write a function to compute whether a given
coordinate is a prefix of another.

\begin{verbatim}
      P←1 1 0 0 0
      C(=∨0=⊢)P
1 1 1 1 1
      ∧⌿C(=∨0=⊢)P
1
\end{verbatim}

Here \verb;P; is the coordinate we wish to check against \verb;C;, to determine
whether \verb;C; is a descendant. The functions are all standard logical
functions extended to apply point-wise over arrays. The function
\verb;⊢; always returns its right argument. The function \verb;(=∨0=⊢);
reads intuitively as, ``equal or right element is zero.'' In this
case to make this a predicate we combine this with the ``for all''
reduction using \verb;∧⌿;. This particular pattern is a special case
of inner product, which we can compute using \verb;f.g;. Thus, \verb;+.×; is a
function that computes traditional matrix-matrix multiplication when
applied to two matrices. However, we can also use it to compute the
above reduction and application:
 
\begin{verbatim}
      C∧.(=∨0=⊢)P
1
\end{verbatim}

This extends the reduction to allow us to use matrix values for
\verb;C; and \verb;P; instead of vectors, and thus determine whether rows in \verb;C;
are descendants of multiple candidates given by \verb;P;. This becomes
important to using the prefix function. In our examples, we want
to determine the nearest ancestor of a certain node type to which a
node belongs. In the case of function lifting, we want to determine
the nearest ancestor function, in the case of expression flattening,
we want to determine the nearest ancestor expression node. We can
extract the rows we care about for each example using \verb;⌷;, where \verb;i⌷M;
returns the i-th row of \verb;M;. In the following expressions, we use \verb;0 2;
rather than \verb;¯1; in \verb;⍤; to indicate that \verb;⌷; will receive the entire
matrix on the right for each scalar index on the left.

\begin{verbatim}
      ⊢Fp←0 2⌷⍤0 2⊢Fc
1 0 0 0 0
1 1 1 0 0
      ⊢Ep←0 1 6⌷⍤0 2⊢Ec
1 0 0
1 1 0
1 3 0
\end{verbatim}

The matrices \verb;Fp; and \verb;Ep; correspond to the function and expression
node coordinates in our two examples, respectively. We could now
use \verb;Fp; and \verb;Ep; to compare against \verb;Fc; and \verb;Ec; and determine which nodes
contained which nodes in the tree. However, our prefix function will
return 1 when we ask whether a node is a prefix of itself. In this
case we don’t want that. The solution to this is to drop the last
non-zero element from the coordinate. This will not prevent it from
matching against any of its ancestors, but will prevent matching
against itself. We can use the depth vector with the \verb;↑; function to
take all but the last non-zero element. We need to remember to extend
the returned array with an extra zero column, since the shape will
be too small otherwise.

\begin{verbatim}
      ⊢Fcp←(Fd↑⍤¯1⊢Fc),0
0 0 0 0 0
1 0 0 0 0
1 1 0 0 0
1 1 1 0 0
1 1 1 1 0
1 1 1 1 0
1 1 1 1 0
1 1 0 0 0
1 1 2 0 0
      ⊢Ecp←(Ed↑⍤¯1⊢Ec),0
0 0 0
1 0 0
1 1 0
1 1 0
1 1 0
1 0 0
1 0 0
1 3 0
1 3 0
1 3 0
\end{verbatim}

We can now use \verb;Fcp; and \verb;Ecp; to determine which Function and Expression
nodes match for each node:

\begin{verbatim}
     Fcp∧.(=∨0=⊢)⍉Fp
0 0
1 0
1 0
1 1
1 1
1 1
1 1
1 0
1 0
      Ecp∧.(=∨0=⊢)⍉Ep
0 0 0
1 0 0
1 1 0
1 1 0
1 1 0
1 0 0
1 0 0
1 0 1
1 0 1
1 0 1
\end{verbatim}

We use the ⍉ function, which computes the transpose of its right
argument, to ensure that the row and column sizes match up. With the
above, we can determine the ``greatest'' match, which is the closest.
We can replace each 1 in the above matrices by their column numbers
(the number of the column in which that 1 occurs), and then we can
take the maximum column number in each row to determine an index in
either \verb;Fp; or \verb;Ep; that is the appropriate ``parent'' coordinate for
that node. This works because we have made sure to order the \verb;Ep; and \verb;Fp;
matrices lexicographically. The following shows this computation for
the expression example; we elide the function example as redundant
at this point. We also use \verb;⌈/; instead of \verb;⌈⌿; to reduce along
the last, rather than the first, axis.

\begin{verbatim}
      ⊢Ei←⌈/(⍳3)×⍤1⊢Ecp∧.(=∨0=⊢)⍉Ep
0 0 1 1 1 0 0 2 2 2
      ⊢Ek←Ei⌷⍤0 2⊢Ep 
1 0 0
1 0 0
1 1 0
1 1 0
1 1 0
1 0 0
1 0 0
1 3 0
1 3 0
1 3 0
\end{verbatim}

At this point we have two values, \verb;Ek; and \verb;Fk;, which indicate the
closest containing node that we care about for each node in the tree,
using its node coordinate.

\begin{verbatim}
      Fk
1 0 0 0 0
1 0 0 0 0
1 0 0 0 0
1 1 1 0 0
1 1 1 0 0
1 1 1 0 0
1 1 1 0 0
1 0 0 0 0
1 0 0 0 0
\end{verbatim}

We will use these keys to perform computation over the AST and
accomplish our tasks of function lifting and expression flattening in
the next section, but we make a final note here, that we can imagine
many other operations which might be used throughout the compiler when
we care about how nodes relate to each other. The use of operators
like reduction, scan, inner and outer products allow us to obtain
the necessary information from the node coordinate matrix. Sometimes
these results might not be obvious, but the example of the ``belongs
to'' relationship given above demonstrates a pattern that arises
repeatedly throughout our data parallel compiler, and is therefore
particularly useful to understand.

\begin{table*}
\centering
\begin{tabular}{l p{2.8in} l l l}
\toprule
Pass & Description & Core Patterns & Coordinate? \\
\midrule
Record Node Coordinates & Adds a new field to each node containing that node's 
 node coordinate. & Outer Product/Scan & Yes \\
Record Function Depths & Adds a new field to each node recording how many 
 functions surround the node. & Inner Product & Yes \\
Drop Unnamed Functions & Eliminates some code that will not be evaluated at 
 the top-level. & Filter & No \\
Drop Unreachable Code & Eliminates some unreachable code. &
 Filter/Inner Product & Yes \\
Lift Functions & Moves all functions to the top-level. & Key & Yes \\
Drop Redundant Nodes & Eliminates unnecessary nodes/nesting & Filter & No \\
Flatten Expressions & Removes nesting from expressions. & Key & Yes \\
Compress Atomic Nodes & Atomizes nested atom nodes & Amend & No \\
Propagate Constants & Inlines all references to literal values. &
 Amend/Rank & Yes \\
Fold Constants & Converts constant expressions to literals & Amend & No \\
Compress Expressions & Converts expression sub-trees into single nodes. & 
 Amend & No \\
Record Final Return Value & Records the value returned by each function. & 
 Key & No \\
Normalize Values Field & Normalizes the shape and size of the values field. &
 Amend & No \\
Lift Type-checking & Infers some type information at compile time &
 Power Limit/Rank & No \\
Allocate Value Slots & Does a form of frame allocation for variables &
 Key & No \\
Anchor Variables & Resolves lexically scoped variables &
 Key & Yes \\
Record Live Variables & Records the variables that are live at each point of 
 execution. & Key & No \\
Fuse Scalar Loops & Identifies Fusion opportunities and fuses expressions &
 Key & No \\
Type Specialization & Specializes each function for a series of potential inputs. 
 & Key & No \\
\end{tabular}
\caption{A listing of some compiler passes in the Co-dfns compiler and their 
 relationship with the Key operator and associated tree computation techniques}
\label{tab:passes}
\end{table*}

\subsection{The Key Operator}

The Key operator (written \verb;⌸;), takes a single function
that expects two arguments and returns a function which takes two
arguments. The left argument is a set of keys, and the right argument
is a set of corresponding elements associated with those keys. In our
case, we provide matrices for both of these arguments, so each pair
of rows corresponds to a key-value pair. A simple example of the key
operation at work is to compute a histogram (\verb;≢; here can be thought
of as ``tally'' or count):

\begin{verbatim}
       ⊢X←?10⍴5
1 1 4 0 1 3 1 1 2 1
      X(⊣,(≢⊢))⌸X
1 6
4 1
0 1
3 1
2 1
\end{verbatim}

To understand a bit better how the Key operator applies its function,
consider the function \verb;{⍺ ⍵}; which returns the pair of its right
and left arguments. If we apply it to the same value as above, we
get the following:

\begin{verbatim}
      X{⍺ ⍵}⌸X
1  1 1 1 1 1 1 
4  4           
0  0           
3  3           
2  2
\end{verbatim}

In our case, we use either \verb;Fk; or \verb;Ek; as our keys applied to the
corresponding AST. We also will drop off the first row in each AST
using \verb;1↓; since this node ``contains'' everything. In a complete
AST this is usually the Module boundary node which contains the entire
set of functions and values in the module.

\section{Case Studies}

The following compiler passes are often found in any suitably complex 
language being compiled, and represent a difficult problem for anyone attempting 
to do data-parallel computation. Indeed, they were the primary motivating 
factor for the development of the node coordinate matrix and its associated 
techniques. They demonstrate how we can operate on highly nested structures, 
usually with the intent of eliminating that structure to more closely map 
the intended output language. 

These case studies come from the Co-dfns compiler, which is compiling a 
lexically scoped language that contains both nested functions and 
expressions. 

\subsection{Function Lifting}

If we use the key operator with \verb;Fk;, we get the following (the monadic
use of \verb;↓; converts \verb;Fc; from a matrix to a vector of vectors):

\begin{verbatim}
      (1↓Fk){⍺ ⍵}⌸1↓Fd,Ft,Fv,⍪↓Fc
┌─────────┬─────────────────┐
│1 0 0 0 0│┌─┬─┬─┬─────────┐│
│         ││1│E│0│1 1 0 0 0││
│         │├─┼─┼─┼─────────┤│
│         ││2│F│0│1 1 1 0 0││
│         │├─┼─┼─┼─────────┤│
│         ││2│A│0│1 1 2 0 0││
│         │├─┼─┼─┼─────────┤│
│         ││3│N│7│1 1 2 2 0││
│         │└─┴─┴─┴─────────┘│
├─────────┼─────────────────┤
│1 1 1 0 0│┌─┬─┬─┬─────────┐│
│         ││3│E│0│1 1 1 1 0││
│         │├─┼─┼─┼─────────┤│
│         ││4│V│⍵│1 1 1 1 1││
│         │├─┼─┼─┼─────────┤│
│         ││4│P│+│1 1 1 1 2││
│         │├─┼─┼─┼─────────┤│
│         ││4│V│⍵│1 1 1 1 3││
│         │└─┴─┴─┴─────────┘│
└─────────┴─────────────────┘
\end{verbatim}

Notice that we have now grouped all of the relevant parts of the
tree according to which nodes would appear in their respective
functions after lifting. Refer to the original lifting example in
the introduction to verify this. Indeed, the second row in the
above example shows the internal function complete and ready to
name. Each element in the second column corresponds to the body of
one of our lifted functions. In the case of the first function, the
outer function, we have a spurious function node in the body. This
is intentional. When we lift these functions, we will replace
each spurious function node with a variable node referring to the
function’s generated name.

Each of these function bodies has a specific coordinate associated
with it. Because these coordinates are uniquely identifying, we can
use these as input into a name generator to generate names that we
know are unique for each function body. Furthermore, because we
retain this information in the corresponding function nodes that
appear in the body of each function to be lifted, we know exactly
what name that function has been given, and we can replace the
function node with a variable node referencing that name instead,
without referring to any state outside of the immediate information
given to the function lifter. Indeed, each row in the above matrix
represents a function lifting task that can be completed without any
additional information. That is, there are no dependencies between
rows to perform lifting. This gives us a straightforward parallel
execution of function lifting.

The final task to complete function lifting of each function
body before lifting is to shift the depths in the depth vectors
to correspond to those of a function lifted to the top-level, and
to attach a function node to the top of each of the bodies. At that
point, we simply recombine all of the newly created functions into
a single top level.

\subsection{Expression Flattening}

If we use \verb;Ek; as the key for the expression example, we get the
following:

\begin{verbatim}
      (1↓Ek){⍺ ⍵}⌸1↓Ed,Et,Ev,⍪↓Ec
┌─────┬─────────────┐
│1 0 0│┌─┬─┬─┬─────┐│
│     ││1│E│0│1 1 0││
│     │├─┼─┼─┼─────┤│
│     ││1│P│÷│1 2 0││
│     │├─┼─┼─┼─────┤│
│     ││1│E│0│1 3 0││
│     │└─┴─┴─┴─────┘│
├─────┼─────────────┤
│1 1 0│┌─┬─┬─┬─────┐│
│     ││2│V│a│1 1 1││
│     │├─┼─┼─┼─────┤│
│     ││2│P│+│1 1 2││
│     │├─┼─┼─┼─────┤│
│     ││2│V│b│1 1 3││
│     │└─┴─┴─┴─────┘│
├─────┼─────────────┤
│1 3 0│┌─┬─┬─┬─────┐│
│     ││2│V│c│1 3 4││
│     │├─┼─┼─┼─────┤│
│     ││2│P│×│1 3 5││
│     │├─┼─┼─┼─────┤│
│     ││2│V│d│1 3 6││
│     │└─┴─┴─┴─────┘│
└─────┴─────────────┘
\end{verbatim}

Again, we can see immediately that we have grouped each set of nodes
according to the expressions that are to be lifted. Just as in the
case of function lifting, we can adjust the depths of each expression
to the correct depth and we can replace each expression node with a
variable reference based on that node’s coordinate. Each expression
can be given a unique name based on its coordinate. A later compiler
pass can reduce these names down to the minimum actually required to
represent the expression if desired.

The only extra issue involved here is to ensure that the order of
evaluation matches. In our case, we are assuming that the order of
evaluation is right to left, which means that the above order is
actually backwards of the desired order. During recombination, we
simply reverse these orders and this fixes that problem. More work
would be required to take into consideration a specific precedence
hierarchy.

\section{Other Passes}

Table \ref{tab:passes} contains a selection of passes from the
Co-dfns compiler and breaks down their features based on how they
traverse the tree and whether or not they use the Key operator or
Node Coordinates. They are oriented with front-end passes higher in
the table and passes further down the compiler chain at the bottom
of the table. These passes give a good idea of how the Key operator
plays a part in the complete compiler as well as how Node Coordinates
help to deal with complex sub-tree selections.

We make a few notes on the table and the analysis. The Key operator
can be used with or without the node coordinates structure. In cases
where the AST is sufficiently flat and arranged in a sufficiently
convenient order, the grouping operations are very simple. In these
cases, the Key operator amounts to a map operation over some selected
sub-trees selected at a specific depth. These occur later in the
compiler where the reader will note appearences of the Key operator
without the use of Node Coordinates.

Likewise, node coordinates find their place early on in the compiler
where it is necessary to deal with the more highly nested AST. They
are useful for identifying this information regardless of whether the
Key operator is used to handle the grouping or not. Thus passes like
``Drop Unreachable Code'' that remove whole sub-trees from the AST
may use the Node Coordinates to do the grouping once the correct
parent node has been identified.

In short, the goal of the structure and design of the compiler passes
given in table \ref{tab:passes} is to utilize Node Coordinates to
remove as quickly as possible the nested structure of the AST into
a flatter form that does not require complex analysis to traverse.

\section{The Co-dfns Compiler}

The Co-dfns compiler is an experimental commercial compiler funded by 
Dyalog, Ltd. It is intended to provide increased scaling and performance
for APL programmers using the dfns syntax (lexically scoped, functionally 
oriented APL). These techniques form one of the foundational elements to 
the architecture of the compiler, which is built in the style of a Nanopass 
compiler, with very small passes chained together through composition, with 
the added requirements that each compiler pass must be fully data parallel. 

The compiler produces good code that performs closely with hand-written 
C code for the same programs on the GPU and CPU. It produces platform 
independent code, meaning that the compiler is able to target either the 
GPU or CPU from the same program source and produce reasonable performance 
on both. 

The compiler itself is composed of three main parts: the core compiler, 
the parser, and the code generator. The code generator includes the runtime 
library of the compiler. Parsing APL code in parallel is a well-studied 
problem. The compiler currently uses parsing combinators to do its parsing, 
though we plan to implement the Two-by-Two parser given in the literature. 
The code generator itself is a simple parallel map over each node, as 
each node may be generated independent from the other nodes, leading to a 
fully parallel generator, though there are some branching elements in the 
generator that still remain, currently. 

The core of the Co-dfns compiler is written entirely using function trains 
and parallel operations in the style described above. We use these techniques 
extensively throughout the compiler, and the code is publicly available 
at our repository. The core of the compiler, including some of the large 
tables comes in at around 250 lines of code. Without the tables, the core 
logic comes in at around 150 lines of code. 

The entire compiler, together with its runtime libraries, parser, generator, 
and core, comes in at around 1500 lines of commented code. 

\section{Future Work}

Work remains on expanding the features and capabilities of the compiler, 
to demonstrate the scalability of these techniques on more sophisticated 
optimizations. The current compiler represents a non-trivial instance of 
the successful application of these techniques, but it focuses on an untyped, 
functionally oriented language. The type inference in the current version 
of the compiler is somewhat naive. More work is required to expand the 
type inferencer to include more sophisticated type inference. Likewise, 
the compiler does not currently handle error handling such as signals or 
exceptions. 

Further work remains to implement these techniques on an imperative and 
object-oriented language instead of a functional one. It would also be 
educational to implement a version for a lazy language. 

The performance of these techniques has not been a problem in practice, but 
we do not have a large scale understanding of the performance characteristics 
of these techniques in real life. Further implementation and analysis is 
required to understand what sort of compiler optimizations and analyses are 
necessary to compile the Co-dfns compiler efficiently for the GPU and CPU. 
We current lack a good comparison against existing techniques for the same 
language, since the current compiler is relatively unique in that no other 
compilers to our knowledge compile the same language to the same targets 
with the same sorts of optimizations. We intend to implement other paradigms 
and other languages which do have existing compiler implementations to compare 
relative performance on multiple platforms. 

While we have found this method of compiler construction to be very compact 
and to permit working with a data-parallel compiler as easily or nearly as 
easily as with a standard Nanopass compiler, we have not done extensive 
studies to determine just how comparable in programmability, ease of 
maintenance, or extensibility these techniques are with established 
compiler construction methods, such as OOP Visitors, Nanopass, or the 
type-directed functional style.

\section{Related Work}

The J programming language \cite{hui2014key} was the first practical,
general-purpose programming language to introduce the key operator as a
primitive operator with the presumption of its general usefulness. The
rank operator (⍤) used throughout this treatment also derives from
the J traditional, receiving particular interest throughout the APL
community \cite{bernecky1987rank,hui1995rank}.

Fritz Henglein demonstrated a class of operations, called
discriminators, of which the Key operator is a member.  
\cite{henglein2013dd}
Namely,
a discriminator performs the same grouping computation as Key, but
does not apply a function over these groups with their keys. Henglein
provides a linear implementation of these operations.

The EigenCFA effort \cite{prabhu2011eigencfa} demonstrated significant performance
improvements of a 0-CFA flow analysis by utilizing similar techniques
to those demonstrated here. In particular, encoding the AST and using
accessor functions have a very similar feel to the node coordinates and
AST encoding given here, though they have a different formulation and
spend considerable effort understanding the trade-offs of performance
associated with the different encodings, whereas the encodings here
were chosen for their clarity and directness, rather than their
performance.

Mendez-Lojo, et al. implemented a GPU version of Inclusion-based
Points-to Analysis \cite{mendez2012inclusion} that also focuses on 
adapting data structures
and algorithms to efficiently execute on the GPU. In particular,
they use similar techniques of prefix sums and sorts to achieve some
of their adaptation to the GPU, Additionally, they have clever and
efficient methods of representing graphs on the GPU which enable
dynamic rewriting of the graph.

The APEX compiler \cite{bernecky1997apex} developed vectorized approaches to handling
certain analyses to compile traditional APL, including a SIMD tokenizer
\cite{bernecky2003tokenizer}.
 It uses a SSA representation, and converts the dynamic scope of
traditional APL functions into a static form early on. It also uses
a matrix format to represent the AST. Traditional APL did not have
nested function definitions, however, and thus the APEX compiler does
not have any specific approaches to dealing with function lifting.

Bernecky further identified methods of reducing or optimizing the
computational complexity or cost of certain array operations, allowing
improved performance of easy to understand array expressions
\cite{bernecky1999reducing}.

Timothy Budd implemented a compiler \cite{budd1984apl,budd2012apl} for APL which targeted
vector processors as well as one for C code. They used a method of
lazy evaluation to avoid intermediate data copying. Budd provided
thoughts and some ideas on how the compiler might be implemented in
parallel as well.

Walter Schwarz implemented an APL to C compiler for the ACORN system
targeting the CM-2 machine, demonstrating performance potential for
APL as a massively parallel language \cite{schwarz1991acorn}.

W. Ching and D. Ju have spent significant work on the ELI language
and other APL-class language
 implementations, especially on parallelized code and optimization. 
\cite{ching2000design,ching1994experimental,ching1993primitive,
ching1990automatic,hendriks1990sparse,ju1991exploitation,
ju1991performance}

J. D. Bunda and J. A. Gerth presented a method for doing table driven
parsing of APL which suggested a parallel optimization for parsing,
but did not elucidate the algorithm \cite{bunda1984apl}.

\section{Conclusion}

We have derived a method of performing computation over sub-trees
selected on the basis of inter-node properties through the use of the
Key (⌸) operator and node coordinates, which enable local computation
of these inter-node properties. This method is both general and direct,
and when combined with traditional and more mundane array programming,
suffices to implement the complete core of a compiler, modulo parsing
and code generation. The method requires no special operations or
unique special casing primitives in the language. Moreover, it is
strictly data-parallel and data-flow, without any complex control flow,
which results in exceptionally concise code. These methods permit a 
general, high-level approach to constructing a compiler that is 
parallel by construction by constricting the language to only a data parallel 
subset of a normal array language. 

We have demonstrated the technique and the core insights behind the
data structures involved. It presents a solution to a very old and
traditional problem in a very uncommon light, by eschewing the common
practices that underlie every other significant and general solution
found in modern compilers today and replacing them with an entirely
different paradigm centred on parallelism and aggregate operations.

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
