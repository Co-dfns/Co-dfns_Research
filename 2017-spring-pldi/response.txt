Thank you to all of the reviewers. There are significant misunderstandings to clarify, importantly, the intent and scope of this paper. We are not writing a compiler for a data parallel language, we are using an existing data parallel language as a means of writing any arbitrary compiler, indeed, any arbitrary set of synergistic tree transformation (as opposed to traversals). To wit, what would it look like to have an entire compiler natively expressed as a data parallel program such that it ran natively and entirely on the GPU? We focus on transformations over traversals since the literature does very little to provide concrete techniques for implementing any arbitrary tree transformation natively on the GPU in a concise form. Our techniques scale to compilers of programming languages, document processing, web processing, database query optimization; we focus our scope on the easily delineated area of classic compiler transformations.

The entire Co-dfns compiler is written in this style, with Table 1 in the paper giving the primary compiler passes. Every pass has already been implemented in this data parallel style, summarized in the Table. Importantly, having implemented the entire compiler in this manner, distinguish code size between it and a compiler of similar complexity written in a traditional compiler-writing DSL (Nanopass). The Co-dfns compiler is 150 LoC compared to the published Nanopass compiler at 2,000 - 4,000 LoC. This is a significant result. Usually, the opposite is true. In the case of Scan, the smallest, cleanest code example given in the literature for a parallel prefix sum (which is expressed in 1 line of C for sequential/CPU versions) is 100 LoC. This is a 100 times increase in code size to move to the GPU. In this paper, we not only can natively express arbitrary compiler transformations on the GPU, but we find an order of magnitude *decrease* in code size when doing so, which is in stark contrast to the status quo for GPU implementations of non-trivial algorithms.

Regarding notation, a critical element is that the notation is not just a mathematical abstraction over the actual implementation. The expressions in the paper are literally the code and full implementation of these ideas in executable form. The entire code is available and explicated in the paper, requiring no additional code. The expressions can be copied into a session (tryapl.org) and executed without modification. The implementations given for expression flattening and function lifting are the actual implementation of these compiler passes. The reader has no need for additional implementation. The notation is a precise template for implementing these algorithms in any programming language from C and beyond.

Full code over traditional mathematical notation avoids any ambiguity for the uninitiated. Readers may not be familiar with data parallel array programming as a "style" for implementing tree transformations. Readers may struggle to replace their traditional techniques with these without the benefit of full and complete code available with copious examples. The development of the path matrix is an example. We would not expect most developers to readily conceive of a data parallel algorithm for generating the path matrix by a simple description alone. With the exposition, it becomes obvious (even seemingly trivial, which is the goal). Readers need not question whether some detail is omitted. It is trivial, almost rote, to convert this code to the reader's own language of choice. We are not aware of any other standard and readily available notation which is simultaneously concise, executable, and semantically generic enough to serve this purpose within the space constraints we face in the paper.

Real world benchmarks are future work, but the synthetic benchmark is important given the scope. These techniques are not specific or limited to traditional compiler transformations and scale to any range of arbitrary tree transformations. A real world benchmark does not clarify behavior across the range of inputs. The synthetic benchmark distills the comparison of pattern matching/recursion vs. key/path matrix to give a clear idea of how they compare. Real world depends on the local domain. We did not have space to provide enough detail on the various passes in a real compiler to provide an adequate and clear picture of what would be benchmarked if we used real APL code as our input. 

The benchmark results drop to zero as we increase the depth/branching factor because we did not test that combination due to the time limitations on running the code of that size (the time on the CPU grew too excessive).

A reviewer requested a plain English description of a path matrix, which can be found in the second full paragraph of section 3.2. The description given by the reviewer is close to the description given in the paper already, but is too specific. We intentionally allow for a wide range of path matrix implementations to suit specific domain requirements. The last sentence in the proposed description about the depth node and the left node is true in the path matrix version given in the paper, but not true in other versions of a path matrix optimized for different cases. 

We are not sure whether reviewer B referring to the performance of "other benchmarks" refers to other compiler passes written in the style given in the paper or the traditional methods. If the reviewer is speaking of the other passes in the Co-dfns compiler, we should note that the entire compiler is written in the same style as the given passes here, and there is no "local" view style programming in the compiler. It is all fully data parallel style.

A reviewer remarks that we are taking a local view regarding compile time being not a primary concern, over programmer productivity and high performance. We are actually concerned with productivity and performance, and are taking a broad, instead of local view. We are presenting a general suite of techniques for the productive and performant implementation of any sort of tree transformation in a natively data parallel, vector processor friendly way that actually reduces code size and provides improved performance. 