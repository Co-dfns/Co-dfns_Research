The APEX compiler [Bernecky 1997] developed vectorized approaches to handling certain analyses to compile traditional APL, including a SIMD tokenizer [Bernecky 2003]. It uses a SSA representation, and converts the dynamic scope of traditional APL functions into a static form early on. It also uses a matrix format to represent the AST. Traditional APL did not have nested function definitions, however, and thus the APEX compiler does not have any specific approaches to dealing with function lifting. Bernecky suggests an approach for introducing parallelism into the compiler itself. While APEX is not self-hosting, its efforts to introduce parallel compiler passes should inform Co-dfns pass design. APEX does suffer from strong restrictions, though some could evaporate given enough effort. Both APEX and Co-dfns share some restrictions, such as not allowing the dynamic fixing of new functions at runtime. However, Co-dfns strives to ensure greater compatibility with dfns programs in Dyalog than analogous programs and APEX. 

Bernecky further identified methods of reducing or optimizing the computational complexity or cost of certain array operations, allowing improved performance of easy to understand array expressions [1999]. 

Timothy Budd implemented a compiler [1984; 1988] for APL which targeted vector processors as well as one for C code. They used a method of lazy evaluation to avoid intermediate data copying. Budd provided thoughts and some ideas on how the compiler might be implemented in parallel as well. Budd's compiler delayed computation until the point at which the program requested the value. As in most traditional APL compilers, the accepted language does not coincide with normal APL programs of the time. Budd's compiler also managed allocation without requiring a garbage collector. 

Walter Schwarz implemented an APL to C compiler for the ACORN system targeting the CM-2 machine, demonstrating performance potential for APL as a massively parallel language [1991]. 

W. Ching and D. Ju have spent significant work on the ELI language and other APL-class language implementations, especially on parallelized code and optimization. [Ching 1990; 2000; Ching, et al. 1993; Ching and Katz 1994; Hendriks and Ching 1990; Ju and Ching 1991; Ju, et al. 1991] 

J. D. Bunda and J. A. Gerth presented a method for doing table driven parsing of APL which suggested a parallel optimization for parsing, but did not elucidate the algorithm [1984]. 

Single-assignment C [Grelck and Scholz 2006] attempts to deliver a high-level, C-like language that uses arrays as first-class data types. It focuses heavily on a functional paradigm and automatically parallelizes code. The same issues of memory copying and array management occur in SAC as in Co-dfns, but SAC is a purely functional language, whereas Co-dfns admits array mutation and variable assignment within a single scope. 

Dyalog has provided a good deal of input into Co-dfns, and so it makes sense that the current release of their interpreter begins to implement some of the ideas in Co-dfns. This includes a facility for doing coarse-grained parallelism with futures, but does not include any ability to do more refined concurrent operations since the interpreter lacks single assignment arrays for synchronization; the interpreter itself makes no guarantees when effects occur in parallel. [Kromberg and Foad 2013] 

The McLAB project [Casey, et al. 2010] implements a compiler and supporting systems for MATLAB code. Like Co-dfns, it provides an explicit intermediate language for work, but goes to a lower level with its own JIT. It also produces Fortran and C code, which are not explicit targets of Co-dfns. The MATLAB language itself differs from Co-dfns, which naturally results in differences of approach with McLAB and Co-dfns. Co-dfns adapts the APL language with explicit parallelism constructs, rather than emphasizing automatic parallelization of the runtime primitives. 

Languages like X10 [Charles, et al. 2005], Fortress [Steele 2006], and Chapel [Chamberlain, et al. 2007] also strive to scale programming to large distributed systems. They inherit much of their linguistic history from Fortran, Java, and C++, rather than APL. They also emphasize a more object-oriented approach than the array-centric, functional approach of Co-dfns. They expose much more explicit syntactic constructs for controlling data layout and synchronization, whereas Co-dfns tries to minimize explicit syntax as much as possible. 

A number of systems such as ZPL [Lin and Snyder 1994] and Accelerate [Chakravarty, et al. 20011] are able to provide interesting implementation strategies for array programming, each emphasizing different elements. They all take the overall approach of altering the language design in favor of making certain features prevalent. Accelerate, for instance, lifts rank to the type level, meaning that shapes are no longer first class entities. ZPL uses a more traditional language but enables predictable data layout for distributed computing. Accelerate takes advantage of the language and implementation to make heavy use of fusion. 

Eric Holk's Harlan system [Holk 2011] takes a unique approach. Targeting the GPU explicitly, it tries to introduce traditional programming concepts as native concepts on the GPU, so that traditional programs can run reasonably on the GPU. This approach, instead of lifting array programming to the general-purpose sphere, pulls general-purpose language constructs such as ADTs into the GPU and array world through the use of region inference and a number of other transformations. It also uses the nanopass style of compiler design and includes a macro system on top of it to reduce the number of core forms the compiler must consider.
