The authors would like to express their gratitude for the reviewers thoughtful responses. We first would like to acknowledge the alien appearance of APL expressions and the challenge that any alien paradigm can present to a reader. We do not wish to claim either that the expressions given in the paper should be considered trivial or obvious to the uninitiated. Much as standard mathematical notation or the use of laziness, static typing, and recursion can present a serious potential challenge to a reader who is unpracticed and unfamiliar with their use, the APL language presents a challenge precisely because it delivers a novel and different approach to solving problems. 

We believe, however, that this novelty, and its associated challenges, creates such remarkably unexpected and surprising results that the challenge is worth undertaking. Indeed, the results are remarkable enough that even without taking the time to acquire a complete understanding of the APL expressions included in our paper, the very nature of their construction, the very fact that they can be put into a paper of this size while expressing what they express, makes them worthy of, at the very least, an increased awareness and publication. That is to say, we believe that a reader can derive significant value from our paper simply by becoming aware of what can be done by approaching problems in this manner, even if the exact details remain somewhat unclear without more effort, much in the same way that one could appreciate the potential of algebraic notation after seeing what is possible with the notation even if the only form of reasoning known at that point has been longform prose and considering the associated time it would take to learn mathematical notation. 

The reviewers have compared the perceived challenge of our APL expressions against the simpler to call PyTorch API. We contest that this strongly misses the point, and is in fact the wrong comparison. They are comparing the "wiring" code for a neural network, which is no more than the threading of data through pre-defined library functions of specific, limited, and constrained scope, against a fully defined, complete, from scratch, zero-library, zero-framework implementation of that same neural network in a programming language that has no notion of neural networks, no notion of machine learning, and was not designed with machine learning in mind. Our implementation more accurately represents a reimplementation of large portions of the PyTorch functionality. It would be more accurate to imagine what it would be like to reimplement PyTorch's functionality from scratch, with the same performance, and then compare that implementation to the APL version. 

We include PyTorch as a reference, but were unable to include a directly comparable reimplementation of PyTorch's functionality in another programming language due to space constraints and difficulty of implementation. The authors have some experience doing such things in various languages, including leveraging "supporting tools" like numpy, to implement even simpler machine learning projects than the U-net architecture described here, but even such simpler implementations with the full support of a system like numpy are far too complex, verbose, and difficult, to say nothing of its performance, for inclusion literally and in full within the body of a conference paper. Even less would be the feasability of including a performant reimplementation of PyTorch's functionality as used in this paper in its totality within the body of a single paper. Nobody would even attempt to do this. 

Yet, with APL, we are able to include 2 copies of every expression, a typeset and a source-style formatting version, in the paper, in full detail, and these expressions implement the entire semantics of a neural network from the ground up in complete detail. Even more, when we compared these APL expressions to the mathematical expressions used in various online resources on neural network programming to describe the same things, our APL expressions were actually shorter and structurally simpler than the standard mathematical expressions for the same concepts!

Thus, these single APL expressions are sufficiently expressive, complete, and concise that they can subsume both the mathematical expressions associated with the operations as well as libraries like PyTorch, while also achieving performance that is "shockingly" close to that of highly tuned and labored over domain-specific libraries such as PyTorch, without a single domain-specific optimization anywhere. 

These results become even more worthy of note by the fact that other than the use of the APL paradigm and its notation there are zero new novel concepts applied. The compiler itself uses no special or novel optimizations. There are no special implementation details, no novel data structures or other programming language semantics. There is no special code generation. The only novel thing is the use of APL as the language/semantics. There are no domain-specific features optimized for neural networks anywhere in the programming stack. 

In a paper of this sort, we cannot hope to provide a full treatment of CNNs, so we have chosen a network of sufficient complexity as well as sufficient history that a reader may find ample discussion of the architecture and possible variations elsewhere, and mimicked its implementation as closely to the standard as possible. Likewise, we cannot hope to turn the reader into an expert at APL within the space of a single paper; the language is too alien for that. 

What we can do is provide a demonstration of the power of the notation and inspire the reader to pursue such ideas in more depth. We intend to provide extended resources that would not fit in the paper for understanding these ideas, but these will not fit within the space limitations of the paper. 

With all of that said, the reviewers have highlighted important points of clarity that will permit us to more fully and simply explain some of the pain points that tripped up the reviewers, and this should greatly simplify the parsing of some of the expressions, though the intuition of the array programming model will still be foreign to some. A few careful simplifications and removals will eliminate many of the specific points brought up by reviewers and simplify the overall paper, while also giving us more space to dedicate to explicating the expressions in more detail. 

...