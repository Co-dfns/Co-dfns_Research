The Co-dfns compiler has a somewhat unique architecture. In line with the project goals, the Co-dfns compiler is designed around "eating our own dog food." In other words, we use the dfns language to implement the compiler, and focus on demonstrating the feasibility of such a task at scale. This motivates a different perspective on compiler construction. 

In particular, the Co-dfns compiler is written as a purely data flow, data parallel, functional program. The core of the compiler is written using nothing but basic function composition over a set of general purpose array operators and vectorized array primitives. There are no explicit branching statements, such as switches, if-statements, or visitor-style design patterns. The compiler uses no recursive or other means of complex control flow. Instead, all control flow is direct and simple function composition. All the basic operators are data-parallel or vectorized, and have parallel semantics. (Note that the Co-dfns language includes recursion, branching, and traditional control flow; we simply choose not to use these features when writing the compiler.)

The compiler is not yet able to self-host, so the interpreter executes the code instead. In this case, the implementations of the primitives are in fact sequential, but the primitive operations themselves do not require this interpretation. The compiler produces vectorized and parallel code for the same primitives as an alternative, particularly on parallel architectures like the Xeon Phi and the NVIDIA GPUs. 

The parser is written using recursive parser combinators, but there is some evidence that the parser can eventually be rewritten into this purely data-parallel style as well. Some parts of the code generator currently use branching control flow, but we are also phasing this out as we continue to develop the compiler. We anticipate that the next version of the compiler will completely eliminate complex control flow from all aspects of the compiler except for the parser combinators used to implement the parser. 

The resulting code exhibits a high degree of concision and uniformity. The core compiler passes are 43 lines of pure dfns that fits onto a single 8.5" Ã— 11" letter sized page with 12 point font. The code itself is currently much denser than other languages, but we believe that the consision gained more than justifies this tersity.

We currently represent the AST as a matrix of one node per row in the matrix. In addition to the node attributes and other information about the node, each node is associated with a depth, which indicates the level at which that node appears in the syntax tree. We order the nodes in the matrix according to a pre-order depth-first traversal of the AST. While this representation does allow us to implement the compiler using the above architecture, it is not clearly the "right" representation. We actively explore new representations for the AST, as changes in the AST representation have far reaching ramifications for the ease of implementing a compiler using only data-parallel primitives.

Critically, the memory management design of the compiler greatly affects its performance in real applications. In line with our desire to integrate with Dyalog APL and, particularly, to reduce the overheads involved for the information expert, we utilize the DWA (Direct Workspace Access) features of Dyalog APL to provide access to the internal memory manager of the APL interpreter. This allows us to operate directly over structures inside the interpreter, rather than introducing a conversion phase in and out of the interpreted environment. Initial versions of the compiler used a separate representation for arrays and handled memory management outside of the DWA infrastructure. Doing so gave advantages for flexibility to the compiler but at a severe performance cost. We discuss these performance costs later on, as they turn out to be the first stumbling block we removed in order to achieve reasonable performance gains inside of the compiler.
