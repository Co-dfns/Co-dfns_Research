#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass acmart
\options format=acmsmall,authordraft
\use_default_options false
\maintain_unincluded_children false
\language english
\language_package default
\inputencoding auto
\fontencoding utf8
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "APL385 Unicode"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts true
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry true
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine natbib
\cite_engine_type authoryear
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #008000
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout ACM Conference
\begin_inset Argument 1
status open

\begin_layout Plain Layout
ICFP'22
\end_layout

\end_inset


\begin_inset Argument 2
status open

\begin_layout Plain Layout
International Conference on Functional Programming
\end_layout

\end_inset


\begin_inset Argument 3
status open

\begin_layout Plain Layout
Sep 11 - Sep 16, 2022
\end_layout

\end_inset


\begin_inset Argument 4
status open

\begin_layout Plain Layout
Ljubljana, Slovenia
\end_layout

\end_inset


\end_layout

\begin_layout Title
U-net CNN in APL
\end_layout

\begin_layout Subtitle
Exploring zero-framework, zero-library machine learning
\end_layout

\begin_layout Author
Aaron W.
 Hsu
\end_layout

\begin_layout Email
aaron@dyalog.com
\end_layout

\begin_layout ORCID
0000-0001-9292-7783
\end_layout

\begin_layout Affiliation
\begin_inset Flex Position
status collapsed

\begin_layout Plain Layout
Researcher
\end_layout

\end_inset


\begin_inset Flex Institution
status collapsed

\begin_layout Plain Layout
Dyalog, Ltd.
\end_layout

\end_inset


\begin_inset Flex City
status collapsed

\begin_layout Plain Layout
Bramley
\end_layout

\end_inset


\begin_inset Flex Country
status collapsed

\begin_layout Plain Layout
United Kingdom
\end_layout

\end_inset


\end_layout

\begin_layout Author
Rodrigo Girão Serrão
\end_layout

\begin_layout Email
rodrigo@dyalog.com
\end_layout

\begin_layout Affiliation
\begin_inset Flex Position
status collapsed

\begin_layout Plain Layout
Consultant
\end_layout

\end_inset


\begin_inset Flex Institution
status collapsed

\begin_layout Plain Layout
Dyalog, Ltd.
\end_layout

\end_inset


\begin_inset Flex City
status collapsed

\begin_layout Plain Layout
Bramley
\end_layout

\end_inset


\begin_inset Flex Country
status collapsed

\begin_layout Plain Layout
United Kingdom
\end_layout

\end_inset


\end_layout

\begin_layout Abstract
TBW
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Specialized machine learning frameworks dominate the present industrial
 and educational spaces for deep learning applications.
 A wide number of higly specialized and highly optimized libraries exist,
 often built on top of one another, to support the modern wave of machine
 learning architectures.
 These systems are often more complex than your typical library, and they
 might even be better classified as their own domain-specific languages
 (DSLs).
 While these libraries have supported the current explosion of machine learning
 developers, a number of issues have emerged.
\end_layout

\begin_layout Standard
First, because of their highly specialized nature, users of these systems
 tend to become experts not in generalized programming or algorithmic skills,
 but specialist toolkits and frameworks around a very specific model of
 computation.
 This specialized nature often mandates dedicated courses and even entire
 academic specializations (even at the undergraduate level) focused on the
 mastery of these particular concepts.
 This can create a sharp fall off of skills transferrence, where machine
 learning experts can use machine learning frameworks effectively, but may
 be underdeveloped and underprepared to handle situations that require a
 broader or more adaptive skillset.
\begin_inset Foot
status open

\begin_layout Plain Layout
To someone who only has a hammer, everything looks like a nail.
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Second, from a pedagogical perspective, when teaching machine learning,
 one may often be able to implement simple networks in a general-purpose
 programming language, but trying to teach machine learning through a typical
 general purpose language can be difficult, because one quickly encounters
 performance and scalability limitations that make any non-trivial and interesti
ng applications likely beyond the competency and endurance of your typical
 student.
 This creates a sharp contrast in which one begins with simple systems that
 can be programmed 
\begin_inset Quotes eld
\end_inset

by hand
\begin_inset Quotes erd
\end_inset

 but quickly transitions to highly opaque and complex frameworks that are
 difficult to understand, modify, or intuit.
 This can result in significant reductions in professional competency.
\end_layout

\begin_layout Standard
Third, if a lack of profound and intuitive understanding of the underlying
 mechanics of a deep learning system continues into professional life, the
 result can be a type of 
\begin_inset Quotes eld
\end_inset

programming by knob turning
\begin_inset Quotes erd
\end_inset

 in which neural networks are programmed via trial and error rather than
 through intentional design.
 Machine Learning as a discipline is already opaque enough, with many cases
 of unintended consequences (citation), without the added dangers inherent
 in this sort of unintentional programming guesswork.
\end_layout

\begin_layout Standard
Fourth, the specificity of machine learning frameworks can result in significant
 amounts of code churn and a reduction in the stability of codebases for
 enterprise use.
 Switching hardware, architectures, operating systems, or the like can create
 unstable conditions in which code must be rewritten, adapted, or thrown
 away entirely.
 Machine learning frameworks are often highly vendor-specific, and even
 those which are more vendor-neutral tend to encode significantly greater
 specificity than is historically warranted for code intended to last for
 any long period of time.
 This almost necessitates higher levels of programmer investment in order
 to keep such systems running over a long period of time.
\end_layout

\begin_layout Standard
Despite the above potential issues, specialist frameworks have proven highly
 effective, in large part because of how important high-performance is to
 the domain of machine learning.
 However, in recent years, general-purpose array programming languages have
 seen a resurgence, and naturally, they have been examined in the light
 of machine learning.
 Such languages were also popular during early exploration of neural network
 programming during the 1980's (citation needed), but performance issues
 of then-current hardware prevented further progression.
\end_layout

\begin_layout Standard
APL, as a general-purpose array programming language, created by Kenneth
 Iverson as an improved mathematical notation (cite), has seen an increase
 in popularity over the past decades, in part because of the renewed interest
 in parallel computation and a wider acceptance of the use of a variety
 of programming languages.
 However, only recently has significant new research into the use of APL
 as a possible implementation language for machine learning begun to surface.
 
\end_layout

\begin_layout Standard
The long history of APL, its origins as an pedagogical tool, and its reputation
 for directness of algorithmic expression (cite knuth) help to address some
 of the concerns above.
 Furthermore, it is one of the most linguistically stable languages, while
 also being exceptionally high level and high performance at the same time
 (cite Co-dfns thesis), making it highly suitable for long lived code as
 well as rapid prototyping.
 Finally, the language itself defaults to a data-parallel semantics, making
 its application to GPU programming an obvious conclusion.
 
\end_layout

\begin_layout Standard
While the above advantages might suggest APL as a terrific tool for machine
 learning, unfortunately, the vast majority of implementations have been
 for the CPU only, and those have usually been entirely interpreted.
 Traditionally, compiler implementors have considered APL a challenging
 language to compile (cite), but recent innovations to the language (particularl
y those with a functional programming focus) have made compilation much
 more tractable, and the Co-dfns compiler now exists as an APL implementation
 with native GPU support (cite).
 
\end_layout

\begin_layout Standard
Given the available APL technology and the parsity of existing materials
 on modern machine learning development in APL, we conducted an exploration
 into the state of the art in APL, both from a language design and a runtime
 implementation perspective.
 To do this, we focused our efforts on the implementation and benchmarking
 of the U-net convolutional neural network (cite).
 This is a popular image segmentation architecture with a particularly interesti
ng U-net design.
 It makes use of a range of popular CNN vocabularies and functions while
 having a clear architecture that is not so simple as to be trivial.
 This makes it an ideal candidate for exploring APL's capabilities.
\end_layout

\begin_layout Standard
We make the following contributions:
\end_layout

\begin_layout Itemize
A complete demonstration in APL of the popular U-net convolutional neural
 network, which is non-trivial in vocabulary and architecture
\end_layout

\begin_layout Itemize
Our U-net implementation is exceptionally simple, concise, transparent,
 and direct
\end_layout

\begin_layout Itemize
Our implementation was written with pure APL and no dependencies, frameworks,
 libraries, or other supporting code outside of the APL implementation
\end_layout

\begin_layout Itemize
A functional programming-friendly approach to neural network design and
 implementation
\end_layout

\begin_layout Itemize
An analysis and examination of the current language features within APL
 that appear relevant to CNNs and machine learning
\end_layout

\begin_layout Itemize
A critical discussion and design comparison of two different approaches
 to supporting convolutions and similar operations in a general-purpose
 array language with a recommendation for future implementation improvements
\end_layout

\begin_layout Itemize
A grounded perspective on the applications of general-purpose array programming
 languages like APL to the machine learning space from professional and
 pedagogical angles and how APL compares to alternative, specialist framework
 approaches
\end_layout

\begin_layout Itemize
Performance results of two modern APL implementations, one compiled and
 the other interpreted, on CPU and GPU hardware against a reference PyTorch
 implementation for U-net
\end_layout

\begin_layout Itemize
Performance observations and microbenchmark comparisons of specialized neural
 network functionality exposed in more general purpose array frameworks
 for GPU programming
\end_layout

\begin_layout Itemize
Specific highlighting of low-hanging fruit for improving the current range
 of APL implementations both in terms of language design and runtime implementat
ion
\end_layout

\begin_layout Itemize
A demonstration of the expressiveness and performance that careful language
 design can enable without the need for complex implementation models or
 theory
\end_layout

\begin_layout Section
Background
\end_layout

\begin_layout Standard
In this section, we provide the relevant background to understand the remainder
 of this paper.
 Work has been published on convolutional neural networks (CNNs) and APL
 before
\begin_inset CommandInset citation
LatexCommand citep
key "bernecky-cnn"
literal "false"

\end_inset

, but these are two subjects that are not often seen together.
 To cater for readers coming from each community, this section provides
 provides some superficial background on CNNs and the relevant literature
 and some background on APL.
 Then, we provide further background information regarding the relevant
 details of CNNs and the way they are used in this paper, aided by the APL
 notation.
\end_layout

\begin_layout Subsection
Convolutional neural networks
\end_layout

\begin_layout Standard
The experiment this paper uses to produce its benchmarks is the reproduction
 of a famous convolutional neural network architecture.
 The use of CNNs in machine learning was widely popularised with the publication
 of a paper
\begin_inset CommandInset citation
LatexCommand citep
key "cnns-imagenet"
literal "false"

\end_inset

 that used CNNs to achieve state-of-the-art performance in labeling pictures
 of the ImageNet
\begin_inset CommandInset citation
LatexCommand citep
key "imagenet"
literal "false"

\end_inset

 challenge.
 However, a proeminent paper from 1998
\begin_inset CommandInset citation
LatexCommand citep
key "cnns-lecun-doc-recognition"
literal "false"

\end_inset

 shows that the modern use of CNNs can be dated farther back.
\end_layout

\begin_layout Standard
The use of convolutional neural networks, as we know them today, builds
 on top of the convolutional layer
\begin_inset CommandInset citation
LatexCommand citep
key "intro-to-cnn"
literal "false"

\end_inset

.
 Convolutional layers receive three-dimensional tensors as input and produce
 three-dimensional tensors as output.
 These inputs have a fixed number of channels
\begin_inset Foot
status open

\begin_layout Plain Layout
\begin_inset Quotes eld
\end_inset

channel
\begin_inset Quotes erd
\end_inset

 typically refers to the leading dimension of these inputs/outputs, a nomenclatu
re that is derived from the fact that CNNs were popularised in the context
 of image processing.
\end_layout

\end_inset

 
\begin_inset Formula $n_{in}$
\end_inset

 which are then transformed into 
\begin_inset Formula $n_{out}$
\end_inset

 channels through means of discrete convolutions with a total of 
\begin_inset Formula $n_{in}\times n_{out}$
\end_inset

 kernels, the learnable parameters of the convolutional layer.
 One of the advantages of CNNs is that, although the total number of kernels
 
\begin_inset Formula $n_{in}\times n_{out}$
\end_inset

 depends on the number of input and output channels, the sizes of the kernels
 are independent of the size of the other two dimensions of the inputs.
 Despite the fact that the main dynamics of a convolutional layer is governed
 by discrete convolution with the learnable kernels, the exact behaviour
 of a convolutional layer depends on layer parameters like the padding and
 the stride used
\begin_inset CommandInset citation
LatexCommand citep
key "conv-arithmetic-guide"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
Given that CNNs were primarily used in image recognition-related tasks,
 convolutional layers were often paired with pooling layers that ease the
 recognition of features over small neighbourhoods
\begin_inset CommandInset citation
LatexCommand citep
key "pooling"
literal "false"

\end_inset

.
 The rationale behind these pooling layers, as seen from an image recognition-re
lated context, can be interpreted as follows: the image features one is
 typically interested in (e.g., the recognition or segmentation of objects,
 or image labeling) are not contained in single pixels of the input images,
 but in regions of said pixels.
 Pooling layers are, thus, employed with the purpose of aggregating low-level
 information that can then be used to recognise the larger features of interest
\begin_inset CommandInset citation
LatexCommand citep
key "pooling"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
In 2015, three authors published a paper
\begin_inset CommandInset citation
LatexCommand citep
key "unet"
literal "false"

\end_inset

 introducing the u-net architecture: a CNN with a non-trivial architecture
 that won several biomedical image segmentation challenges when introduced.
 Since its publication, the u-net architecture was reimplemented hundreds
 of times
\begin_inset Foot
status open

\begin_layout Plain Layout
As noted by 
\begin_inset CommandInset href
LatexCommand href
name "Papers with Code"
target "https://paperswithcode.com/paper/u-net-convolutional-networks-for-biomedical"
literal "false"

\end_inset

 as of March, 2022.
\end_layout

\end_inset

, most notably through the use of deep-learning frameworks such as PyTorch
\begin_inset CommandInset citation
LatexCommand citep
key "pytorch"
literal "false"

\end_inset

 or Caffe
\begin_inset CommandInset citation
LatexCommand citep
key "caffe"
literal "false"

\end_inset

 (in fact, the original u-net architecture resorted to one such framework,
 Caffe).
 For this paper, we reimplemented the u-net architecture, in APL, without
 making use of any (machine learning) libraries of frameworks.
 Now, we provide some background on APL, to ease the reader into the notation.
\end_layout

\begin_layout Subsection
APL notation
\end_layout

\begin_layout Standard
The intricate details of the behaviour of convolutional layers, the pooling
 layers, and other details of the u-net architecture were delayed because
 both the authors and the readers will benefit greatly from being able to
 use APL to convey the relevant ideas.
 APL
\begin_inset CommandInset citation
LatexCommand citep
key "apl"
literal "false"

\end_inset

 is a mathematical notation introduced by Turing award winner Kenneth E.
 Iverson, that has since evolved into an executable mathematical notation
\begin_inset CommandInset citation
LatexCommand citep
key "apl-since-78"
literal "false"

\end_inset

.
 In other words, APL is an alternative mathematical notation that can be
 seen as a programming language.
 APL being an 
\begin_inset Quotes eld
\end_inset

alternative
\begin_inset Quotes erd
\end_inset

 mathematical notation doesn't mean it changes everything the reader will
 be accustomed to; in fact, the most well-established parts of the notation
 were left intact, as can be seen by the two following examples of addition
 and multiplication, respectively:
\end_layout

\begin_layout Verbatim

      1 + 2
\end_layout

\begin_layout Verbatim

3
\end_layout

\begin_layout Verbatim

      73 × 104
\end_layout

\begin_layout Verbatim

7592
\end_layout

\begin_layout Standard
The format of the two examples above will be the same throughout the paper
\begin_inset Foot
status open

\begin_layout Plain Layout
This format mimics that of the APL session, the interactive environment
 in which one can use APL.
\end_layout

\end_inset

: the notation typed by the user is indentend to the right and the computed
 result is left-aligned on the following line(s).
 Subtraction and division are also represented by the usual glyphs:
\end_layout

\begin_layout Verbatim

      10 - 1 2 3
\end_layout

\begin_layout Verbatim

9 8 7
\end_layout

\begin_layout Verbatim

      100 50 20 ÷ 2
\end_layout

\begin_layout Verbatim

50 25 10
\end_layout

\begin_layout Standard
In APL, one is allowed to write multiple values next to each other, which
 are then stranded together and interpreted as a vector.
 Thus, 
\family typewriter
1 2 3
\family default
 represents the three-item vector whose elements are the first three positive
 integers.
 Then, the APL function 
\begin_inset Quotes eld
\end_inset

minus
\begin_inset Quotes erd
\end_inset

 takes the scalar 
\family typewriter
10
\family default
 as its left argument and the three-item vector 
\family typewriter
1 2 3
\family default
 as its right argument, and it subtracts each of the items of the right
 argument vector from its left argument.
 Similarly, the division example shows that vectors can also be used as
 the left argument.
 The natural progression is to wonder whether vectors can be used on the
 left and on the right of a function, and typically they can:
\end_layout

\begin_layout Verbatim

      1 10 100 1000 ⌈ 5 5 500 500
\end_layout

\begin_layout Verbatim

5 10 500 1000
\end_layout

\begin_layout Standard
The upstile glyph 
\family typewriter
⌈
\family default
 stands for the 
\begin_inset Formula $max$
\end_inset

 function when used dyadically, that is, with two arguments: one on the
 left and one on the right.
 The result obtained above would be equivalent to computing the four maximum
 operations one by one, and then stranding them together:
\end_layout

\begin_layout Verbatim

      (1 ⌈ 5) (10 ⌈ 5) (100 ⌈ 500) (1000 ⌈ 500)
\end_layout

\begin_layout Verbatim

5 10 500 1000
\end_layout

\begin_layout Standard
Notice how each element of the left vector 
\family typewriter
1 10 100 1000
\family default
 got paired up with the respective element of the right vector 
\family typewriter
5 5 500 500
\family default
.
\end_layout

\begin_layout Standard
The dyadic functions 
\emph on
plus
\emph default
 
\family typewriter
+
\family default
, 
\emph on
minus
\emph default
 
\family typewriter
-
\family default
, 
\emph on
times
\emph default
 
\family typewriter
×
\family default
, 
\emph on
divide
\emph default
 
\family typewriter
÷
\family default
, and 
\emph on
max
\emph default
 
\family typewriter
⌈
\family default
, all share the property that allows them to accept vectors as arguments:
 they are 
\emph on
scalar functions
\emph default
.
 Scalar functions are functions that pervade the structure of the argument(s)
 and apply directly to each of the scalars that make up said argument(s).
 This becomes increasingly relevant when one understands that APL has first-clas
s support for arrays of any dimension: 
\emph on
scalars
\emph default
 (arrays with no dimensions), 
\emph on
vectors
\emph default
 (arrays with a single dimension), 
\emph on
matrices
\emph default
 (arrays with two dimensions), 
\emph on
cuboids
\emph default
 (arrays with three dimensions), and more.
 In the examples above, we have seen scalars such as 
\family typewriter
10
\family default
 and 
\family typewriter
73
\family default
, and vectors such as 
\family typewriter
1 10 100 1000
\family default
.
 Scalars and vectors are the only types of arrays that can be typed directly.
 Arrays of higher dimensions must be loaded from an external data source
 or dynamically created by other means such as the 
\emph on
reshape
\emph default
 function.
\end_layout

\begin_layout Standard
The reshape function is represented by the Greek letter rho 
\family typewriter
⍴
\family default
 and is a dyadic function that reshapes its right argument to have the shape
 specified by the left argument.
 For instance, if we want to create a 
\begin_inset Formula $2\times3$
\end_inset

 matrix with the first six non-negative integers, we can do it like so:
\end_layout

\begin_layout Verbatim

      2 3 ⍴ 0 1 2 3 4 5
\end_layout

\begin_layout Verbatim

0 1 2
\end_layout

\begin_layout Verbatim

3 4 5
\end_layout

\begin_layout Standard
The left argument to the reshape function determines the final shape of
 the result, where each non-negative integer specifies the length of the
 result along the specified dimension.
 So, if the left argument had been 
\family typewriter
5 9 7
\family default
, the resulting array would have been a cuboid (array with three dimensions)
 composed of 
\family typewriter
5
\family default
 planes, 
\family typewriter
9
\family default
 rows, and 
\family typewriter
7
\family default
 columns, holding a total of 
\begin_inset Formula $5\times9\times7=315$
\end_inset

 elements.
\end_layout

\begin_layout Standard
One of the key propositions of APL is to be a notation that is completely
 unambiguous.
 This materialises in some key differences with traditional mathematical
 notation, namely:
\end_layout

\begin_layout Itemize
in traditional mathematical notation, the symbol 
\begin_inset Formula $-$
\end_inset

 is used for subtraction and to indicate negative numbers.
 However, in APL, we use the high-minus 
\family typewriter
¯
\family default
 to represent negative numbers.
 If we didn't, it would be unclear whether 
\family typewriter
1 -3 5
\family default
 represented a three-element vector with the numbers one, minus three, and
 five, or the result of the subtraction of 
\family typewriter
1
\family default
 and the two-element vector 
\family typewriter
3 5
\family default
.
 APL disambiguates the two by representing the former case with 
\family typewriter
1 ¯3 5
\family default
 and the latter with 
\family typewriter
1 - 3 5;
\end_layout

\begin_layout Itemize
in traditional mathematical notation, the symbol 
\begin_inset Formula $=$
\end_inset

 is used for equality comparison and to assign values to variables.
 However, in APL, we use the left arrow 
\family typewriter
←
\family default
 to represent assignment.
 If we didn't, it would be unclear whether 
\family typewriter
x = 10
\family default
 meant we wanted to assign ten to the variable 
\family typewriter
x
\family default
, or if it meant we wanted to compare the value of the variable 
\family typewriter
x
\family default
 with ten.
 Assigning ten to 
\family typewriter
x
\family default
 is written as 
\family typewriter
x ← 10
\family default
 whereas equality comparison is written as 
\family typewriter
x = 10
\family default
.
\end_layout

\begin_layout Standard
Another key difference between APL and mathematical notation (which also
 translates into a key difference between APL and other programming languages)
 is that APL normalises precedence rules to a single level: all functions
 have the same precedence.
 In APL, functions are said to have a 
\emph on
long right scope
\emph default
 and a 
\emph on
short left scope
\emph default
, which is why APL is loosely said to 
\begin_inset Quotes eld
\end_inset

execute from right to left
\begin_inset Quotes erd
\end_inset

.
 A long right scope means that a function takes as right argument everything
 to its right, whereas a short left scope means that a function only takes
 as left argument the array that is immediately to its left.
\end_layout

\begin_layout Standard
Things I still need to go through:
\end_layout

\begin_layout Itemize
symbols as monadic/dyadic functions
\end_layout

\begin_layout Itemize
operators modify functions
\end_layout

\begin_layout Itemize
long right scope vs short left scope
\end_layout

\begin_layout Section
Implementation
\end_layout

\begin_layout Subsection
Overview
\end_layout

\begin_layout Standard
Our implementation of U-net can be roughly divided into two significant
 considerations: the implementation of the fundamental vocabulary of neural
 networks, and the wiring of those operations into the actual U-net architecture.
 We leveraged significant features of APL to implement both aspects of the
 system, and so we will treat each in their own sub-section.
 
\end_layout

\begin_layout Standard
Additionally, because Co-dfns does not yet support the complete array of
 Dyalog primitives and their semantics, some of the implementation techniques
 that we use could be significantly enhanced through the use of a more rich
 feature-set.
 The effect of using these richer features is an increase in concision and
 clarity, but we expect that such improvements would not significantly affect
 the overall performance of the code, either positively or negatively.
 We believe that the overall structure of the code is clear and simple enough
 at the current state of Co-dfns to warrant inclusion almost verbatim here,
 rather than use the richer features and require the reader to translate
 those into the Co-dfns supported feature set in order to execute them.
\end_layout

\begin_layout Standard
One area that deserves particular attention is the design of APL as a language
 itself and the specific features that immediately present themselves as
 particularly well-suited to expressing neural network computations.
 Our exploration of these features uncovered a particular design tension
 that is worth discussing in detail.
 A complete copy of the code discussed in this paper is included in the
 appendix.
 
\end_layout

\begin_layout Subsection
Design of APL Primitives for Neural Networks
\end_layout

\begin_layout Standard
The majority of APL primitives find fundamental use in computing neural
 networks, which isn't surprising given the array-oriented and numerical
 nature of the domain.
 However, the Stencil operator, introduced in Dyalog APL (citation), stands
 out as the most obviously aligned with convolutional neural networks.
 The J programming language introduced an alternative implementation of
 the stencil operator earlier (citation), from which Dyalog derived inspiration
 for the implementation of their own stencil operator.
 
\end_layout

\begin_layout Standard
The stencil operator takes a function left operand and a specification array
 as a right operand.
 Given a function 
\begin_inset Formula $f$
\end_inset

 as the left operand and a specification 
\begin_inset Formula $s$
\end_inset

 as the right operand, the stencil operator, written 
\begin_inset Formula $f\texttt{⌺}s$
\end_inset

, evaluates to a function that applies 
\begin_inset Formula $f$
\end_inset

 on each sliding window specified by 
\begin_inset Formula $s$
\end_inset

.
 The two most common sliding window sizes for stencil in U-net are 
\begin_inset Formula $3\,3$
\end_inset

 for the convolutions, corresponding to a window size of 
\begin_inset Formula $3×3$
\end_inset

 and a step of 
\begin_inset Formula $1$
\end_inset

 for each dimension, and 
\begin_inset Formula $2\,2\texttt{⍴}2$
\end_inset

 for the max pooling layers and up convolutions, corresponding to a 
\begin_inset Formula $2×2$
\end_inset

 window size and a step of 
\begin_inset Formula $2$
\end_inset

 for each dimension.
 
\end_layout

\begin_layout Standard
When first implementing a convolution, almost everyone familiar with Dyalog
 APL and the Stencil operator immediate comes to some variation of the following
 expression for convolving a matrix 
\begin_inset Formula $M$
\end_inset

 with kernel 
\begin_inset Formula $K$
\end_inset

:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\{+/,K×\texttt{⍵}\}\texttt{⌺}3\,3⊢M
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
Recall that 
\begin_inset Formula $K×\texttt{⍵}$
\end_inset

 is the pointwise multiplication of kernel 
\begin_inset Formula $K$
\end_inset

 with one of the 
\begin_inset Formula $3×3$
\end_inset

 sliding windows.
 We write 
\begin_inset Formula $+/,A$
\end_inset

 to indicate the sum of all elements of array 
\begin_inset Formula $A$
\end_inset

.
 Thus, the above stencil computes the 2-D convolution over a matrix with
 a given 2-D kernel.
 Because APL's stencil operator is 
\emph on
leading axis biased
\emph default
, if we were to instead provide a kernel 
\begin_inset Formula $K$
\end_inset

 with shape 
\begin_inset Formula $3\,3\,C$
\end_inset

 where 
\begin_inset Formula $C$
\end_inset

 is the number of channels in an array 
\begin_inset Formula $A$
\end_inset

 of shape 
\begin_inset Formula $M\,N\,C$
\end_inset

, the above expression would still function appropriately.
 However, if we wish to continue to extend this to multiple kernels, that
 is, multiple output channels, it is less straightforward to compute.
 The following expression computes the convolution of an array 
\begin_inset Formula $A$
\end_inset

 with shape 
\begin_inset Formula $M N I$
\end_inset

 using kernels 
\begin_inset Formula $K$
\end_inset

 with the shape 
\begin_inset Formula $O 3 3 I$
\end_inset

 where 
\begin_inset Formula $I$
\end_inset

 is the number of input channels and 
\begin_inset Formula $O$
\end_inset

 the number of output channels:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
K\{k←\texttt{⍺}\,\texttt{⋄}\,\{+/,k×\texttt{⍵}\}\texttt{⌺}3\,3⊢\texttt{⍵}\}\texttt{⍤}3⊢A
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
The result of the above expression is an array of shape 
\begin_inset Formula $O M N$
\end_inset

.
 We make use here of the Rank operator.
 The expression 
\begin_inset Formula $K f\texttt{⍤}3⊢A$
\end_inset

 will divide 
\begin_inset Formula $K$
\end_inset

 and 
\begin_inset Formula $A$
\end_inset

 into subarrays each of rank 3, that is, each with 3 dimensions, and apply
 
\begin_inset Formula $f$
\end_inset

 to the corresponding subarrays of 
\begin_inset Formula $K$
\end_inset

 and 
\begin_inset Formula $A$
\end_inset

.
 Thus, in our expression above, our convolution will be applied over the
 entire 
\begin_inset Formula $A$
\end_inset

 for each output channel described by the first axis of 
\begin_inset Formula $K$
\end_inset

, thus applying the original 2-D convolution over arbitrary numbers of input
 channels and output channels.
 
\end_layout

\begin_layout Standard
Unfortunately, the above expression has a number of design flaws.
 Firstly, the output has the channel count as the leading axis, while the
 expected input is to have the channel count as the trailing axis.
 This requires that we perform a transpotion of these dimensions after computing
 the convolution in order to return our result to the input format.
 Furthermore, the nested structure of the computation results in two primary
 functions of non-primitive complexity, meaning that a more sophisticated
 analysis of this function would be required by a compile-time or run-time
 implementation in order to recognize this code.
 
\end_layout

\begin_layout Standard
On principle, APL is at its best when it can concisely describe operations
 over large arrays at a time, or large sub-arrays at a time.
 In particular, concise APL expressions are possible when the solution can
 be expressed as a composition of basic APL primitives.
 However, in the case of the Stencil operator, almost all interesting use
 cases of the function come from complex, non-simple, non-primitive left
 operands.
 This is further exacerbated by the need to nest the stencil operation in
 an outer rank as above.
 Additionally, the input sizes provided to the left operand of the stencil
 operator are remarkably small, all things considered.
 This guarantees that a naïve implementation of stencil will be inefficient
 and slow, especially on interpreters.
 
\end_layout

\begin_layout Standard
Dyalog APL can mitigate some of these issues through the use of idiom recognitio
n.
 However, we argue that idiom recognition scales particularly poorly to
 this case.
 Idiom recognition has been implemented for the stencil operator, and there
 are a selected number of left operand inputs that are treated specially,
 so that their performance can be enhanced behind the scenes.
 However, because of the complex nature of the left operand inputs, recognizing
 the useful idioms for the Stencil operator is a particularly difficult
 task, and does not scale well to these sorts of problems.
 For instance, while the above stencil operator is considered an idiom,
 the following is not:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
\{⌈⌿⌈⌿\texttt{⍵}\}\texttt{⌺}(2 2\texttt{⍴}2)⊢A
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
This expression is one way to implement part of the max pooling layers in
 U-net.
 However, there are many other obvious variations of this expression that
 would also have to be considered for idiom recognition, which would otherwise
 be missed even if this particular expression were handled.
 In the case of the design of the Stencil operator, trying to improve its
 performance via idiom recognition or even compiler optimization is a relatively
 significant task.
 It is combinatorial and not-compositional.
 
\end_layout

\begin_layout Standard
The result is that significant amounts of code would have to be implemented
 and maintained in order to support performance enhancements on the Stencil
 operator, with none of that work benefiting other parts of an APL runtime.
 
\end_layout

\begin_layout Standard
We instead propose an alternative that was first suggested to us by the
 late Roger Hui, the Stencil 
\emph on
function
\emph default
.
 The stencil function is a function whose left argument is the same as the
 right operand of the stencil operator, and which receives the same right
 argument as the right argument to the function returned by the stencil
 operator.
 A reasonable definition of the stencil function might be:
\end_layout

\begin_layout Standard
\begin_inset Formula 
\begin{equation}
SF←\{\{\texttt{⍵}\}\texttt{⌺}\texttt{⍺}⊢\texttt{⍵}\}
\end{equation}

\end_inset


\end_layout

\begin_layout Standard
We found that using the stencil function was in fact, not only easier to
 work with and more compositional than the stencil operator, but that it
 was also universally faster.
 That is, even with the idiom recognition that Dyalog has put into their
 interpreter to handle the special cases of the stencil operator, of which
 
\begin_inset Formula $SF$
\end_inset

 is one, using the stencil function instead of the stencil operator was
 always at least as fast or faster, despite idiom recognition for the more
 complex uses of the stencil operator.
 
\end_layout

\begin_layout Standard
The compositionality of 
\begin_inset Formula $SF$
\end_inset

 has performance ramifications, since it is fundamentally an indexing operation,
 rather than a computational operation.
 This categorical shift means that it can now be approached using the same
 sorts of lazy indexing and fusion operations that are common for other
 indexing operations, such as transposition.
 This means that the use of the stencil function can help to broadly reduce
 intermediate array generation, and performance enhancements to indexing
 will compose well with 
\begin_inset Formula $SF$
\end_inset

.
 Functions and operators that are already designed to fuse with lazily indexing
 functions can then readily take advantage of such features to work with
 the output of 
\begin_inset Formula $SF$
\end_inset

 as well, granting performance enhancements across a wider range of applications
 without ever implementing any idiom recognition, which reduces the amount
 of specialized code that needs to exist as well as the programmer burden
 to maintain such code.
 
\end_layout

\begin_layout Standard
To explore this further, a naïve implementation of the stencil function
 that did not pad its results was implemented in Co-dfns and used in the
 following implementations.
 In the following sections, we use 
\begin_inset Formula $\texttt{⌺}$
\end_inset

 to mean the stencil 
\emph on
function
\emph default
 and not the stencil operator as it appears in Dyalog APL.
 
\end_layout

\begin_layout Subsection
Neural Network Vocabulary
\end_layout

\begin_layout Verbatim

CV←{0⌈z⊣Z[⍺]←⊂Z[⍺],⊂z←(,[2+⍳3]3 3 SF⊃Z[⍺]←⊂⍵)+.×,[⍳3]⍺⊃W}
\end_layout

\begin_layout Verbatim

CC←{⍵,⍨(⌊p)↓(-⌈p)↓(⍺⊃Z)⊣p←2÷⍨(⍴⍺⊃Z)-⍴⍵} 
\end_layout

\begin_layout Verbatim

MX←{⌈⌿[2],[2 3](2 2⍴2)SF⊃Z[⍺]←⊂⍵} 
\end_layout

\begin_layout Verbatim

UP←{((2×¯1↓⍴⍵),¯1↑⍴⍺⊃W)⍴0 2 1 3 4⍉⍵+.×⍺⊃W⊣Z[⍺]←⊂⍵} 
\end_layout

\begin_layout Verbatim

C1←{1E¯8+z÷[⍳2]+/z←*z-[⍳2]⌈/z←⍵+.×⍺⊃W⊣Z[⍺]←⊂⍵}
\end_layout

\begin_layout Verbatim

∆CV←{w←,[⍳3]⊖⌽[1]0 1 3 2⍉⍺⊃W ⋄ x←⊃⍺⊃Z ⋄ ∆z←⍵×0<1⊃⍺⊃Z
\end_layout

\begin_layout Verbatim

  ∆Z←¯2⊖¯2⌽[1](4+2↑⍴∆z)↑∆z
\end_layout

\begin_layout Verbatim

  _←⍺ ∆ 3 0 1 2⍉(⍉,[⍳2]∆z)+.×,[⍳2]3 3 SF x
\end_layout

\begin_layout Verbatim

  w+.×⍨,[2+⍳3]3 3 SF ∆Z}   
\end_layout

\begin_layout Verbatim

∆CC←{x←⍺⊃Z ⋄ ∆z←⍵ ⋄ d←-⌊2÷⍨2↑(⍴x)-⍴∆z ⋄ (⊃d)⊖(1⊃d)⌽[1](⍴x)↑∆z}   
\end_layout

\begin_layout Verbatim

∆MX←{x←⍺⊃Z ⋄ ∆z←⍵ ⋄ y×x=y←(⍴x)↑2⌿2/[1]∆z}
\end_layout

\begin_layout Verbatim

∆UP←{w←⍺⊃W ⋄ x←⍺⊃Z ⋄ ∆z←⍵ ⋄ _←⍺ ∆(⍉,[⍳2]x)+.×,[⍳2](2 2⍴2)SF ∆z
\end_layout

\begin_layout Verbatim

  (,[2+⍳3](2 2⍴2)SF ∆z)+.×⍉⍪w}
\end_layout

\begin_layout Verbatim

∆C1←{w←⍺⊃W ⋄ x←⍺⊃Z ⋄ ∆z←⍵ ⋄ _←⍺ ∆(⍉,[⍳2]x)+.×,[⍳2]∆z ⋄ ∆z+.×⍉w} 
\end_layout

\begin_layout Subsection
U-net Architecture
\end_layout

\begin_layout Verbatim

FWD←{Z⊢←(≢W)⍴⊂⍬
\end_layout

\begin_layout Verbatim

  ⍝ Forward propagation layers ...
\end_layout

\begin_layout Verbatim

  LA←{⍺≥≢Z:⍵
\end_layout

\begin_layout Verbatim

    down←(⍺+6)∇(⍺+2)MX(⍺+1)CV(⍺+0)CV ⍵
\end_layout

\begin_layout Verbatim

    (⍺+2)CC(⍺+5)UP(⍺+4)CV(⍺+3)CV down}
\end_layout

\begin_layout Verbatim

  2 C1 1 CV 0 CV 3 LA ⍵⍴⍨3↑1,⍨⍴⍵}
\end_layout

\begin_layout Verbatim

\end_layout

\begin_layout Verbatim

BCK←{Y←⍺ ⋄ Y∆←⍵
\end_layout

\begin_layout Verbatim

  ∆←{0⊣W[⍺]←⊂(⍺⊃W)-LR×⊃V[⍺]←⊂⍵+MO×(⍴⍵)⍴⍺⊃V}
\end_layout

\begin_layout Verbatim

  ⍝ Back propagation layers ...
\end_layout

\begin_layout Verbatim

  ∆LA←{⍺≥≢Z:⍵
\end_layout

\begin_layout Verbatim

    down←(⍺+6)∇(⍺+3)∆CV(⍺+4)∆CV(⍺+5)∆UP ⍵↑[2]⍨-2÷⍨⊃⌽⍴⍵
\end_layout

\begin_layout Verbatim

    (⍺+0)∆CV(⍺+1)∆CV(⍵ ∆CC⍨⍺+2)+(⍺+2)∆MX down}
\end_layout

\begin_layout Verbatim

  3 ∆LA 0 ∆CV 1 ∆CV 2 ∆C1 Y∆-(~Y),[1.5]Y}
\end_layout

\begin_layout Verbatim

\end_layout

\begin_layout Verbatim

E←{-+⌿,⍟(⍺×⍵[;;1])+(~⍺)×⍵[;;0]}
\end_layout

\begin_layout Verbatim

\end_layout

\begin_layout Verbatim

RUN←{Y Y∆(Y E Y∆)⊣(Y←⌊0.5+nm↑⍵↓⍨2÷⍨(⍴⍵)-nm←2↑⍴Y∆)BCK⊢Y∆←FWD ⍺}
\end_layout

\begin_layout Section
Performance
\end_layout

\begin_layout Subsection
Overview
\end_layout

\begin_layout Subsection
APL Stencil Primitives
\end_layout

\begin_layout Subsection
U-net Performance
\end_layout

\begin_layout Subsection
Microbenchmarks against other libraries
\end_layout

\begin_layout Section
Discussion
\end_layout

\begin_layout Standard
Things to consider in the discussion (my own points of view):
\end_layout

\begin_layout Itemize
something like PyTorch has very low barrier to entry, lets people do machine
 learning with little insight into what is actually being done VS something
 like the APL code written has a higher barrier to entry but allows full
 and effortless control of what's being done
\end_layout

\begin_layout Itemize
APL is great for those wanting to go off the beaten track and using APL
 gets you insights into how things work
\end_layout

\begin_layout Itemize
reading the APL code lets readers see exactly how things are done; in other
 words, no implementation detail is hidden, so reproducing the results is
 easier
\end_layout

\begin_layout Itemize
studying the APL implementation led to a material influence on how the PyTorch
 reference implementation was written
\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Standard
TBW
\end_layout

\begin_layout Acknowledgments
The authors would like to acknowledge the Cell Tracking Challenge
\begin_inset CommandInset citation
LatexCommand citep
key "data-source-ctc"
literal "false"

\end_inset

 as the data source
\begin_inset Foot
status open

\begin_layout Plain Layout
http://celltrackingchallenge.net/2d-datasets/
\end_layout

\end_inset

 and the data providers that granted permission for their datasets to be
 used:
\end_layout

\begin_layout Itemize
the glioblastoma-astrocytoma U373 cells on a polyacrylamide substrate dataset
 was provided by Dr.
 S.
 Kumar from the Department of Bioengineering
\begin_inset Foot
status open

\begin_layout Plain Layout
https://bioeng.berkeley.edu/
\end_layout

\end_inset

, University of California at Berkeley, Berkeley CA (USA); and
\end_layout

\begin_layout Itemize
the HeLa cells on a flat glass dataset was provided by Dr G.
 van Cappellen from the Erasmus Medical Center
\begin_inset Foot
status open

\begin_layout Plain Layout
https://erasmusoic.nl/
\end_layout

\end_inset

, Rotterdam, The Netherlands.
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "bibliography"
options "acm"

\end_inset


\end_layout

\begin_layout Section*
Appendix
\end_layout

\begin_layout Verbatim

:Namespace UNET
\end_layout

\begin_layout Verbatim

\end_layout

\begin_layout Verbatim

 W←⍬ ⋄ V←⍬ ⋄ Z←⍬ ⋄ LR←1e¯9 ⋄ MO←0.99
\end_layout

\begin_layout Verbatim

\end_layout

\begin_layout Verbatim

 FWD←{Z⊢←(≢W)⍴⊂⍬
\end_layout

\begin_layout Verbatim

  CV←{0⌈z⊣Z[⍺]←⊂Z[⍺],⊂z←(,[2+⍳3]3 3⌺⊃Z[⍺]←⊂⍵)+.×,[⍳3]⍺⊃W}
\end_layout

\begin_layout Verbatim

  CC←{⍵,⍨(⌊p)↓(-⌈p)↓(⍺⊃Z)⊣p←2÷⍨(⍴⍺⊃Z)-⍴⍵}
\end_layout

\begin_layout Verbatim

  MX←{⌈⌿[2],[2 3](2 2⍴2)⌺⊃Z[⍺]←⊂⍵}
\end_layout

\begin_layout Verbatim

  UP←{((2×¯1↓⍴⍵),¯1↑⍴⍺⊃W)⍴0 2 1 3 4⍉⍵+.×⍺⊃W⊣Z[⍺]←⊂⍵}
\end_layout

\begin_layout Verbatim

  C1←{1E¯8+z÷[⍳2]+/z←*z-[⍳2]⌈/z←⍵+.×⍺⊃W⊣Z[⍺]←⊂⍵}
\end_layout

\begin_layout Verbatim

  LA←{⍺≥≢Z:⍵
\end_layout

\begin_layout Verbatim

    down←(⍺+6)∇(⍺+2)MX(⍺+1)CV(⍺+0)CV ⍵
\end_layout

\begin_layout Verbatim

    (⍺+2)CC(⍺+5)UP(⍺+4)CV(⍺+3)CV down}
\end_layout

\begin_layout Verbatim

  2 C1 1 CV 0 CV 3 LA ⍵⍴⍨3↑1,⍨⍴⍵}
\end_layout

\begin_layout Verbatim

\end_layout

\begin_layout Verbatim

 BCK←{Y←⍺ ⋄ Y∆←⍵
\end_layout

\begin_layout Verbatim

  ∆←{0⊣W[⍺]←⊂(⍺⊃W)-LR×⊃V[⍺]←⊂⍵+MO×(⍴⍵)⍴⍺⊃V}
\end_layout

\begin_layout Verbatim

  ∆CV←{w←,[⍳3]⊖⌽[1]0 1 3 2⍉⍺⊃W ⋄ x←⊃⍺⊃Z ⋄ ∆z←⍵×0<1⊃⍺⊃Z
\end_layout

\begin_layout Verbatim

   ∆Z←¯2⊖¯2⌽[1](4+2↑⍴∆z)↑∆z
\end_layout

\begin_layout Verbatim

   _←⍺ ∆ 3 0 1 2⍉(⍉,[⍳2]∆z)+.×,[⍳2]3 3⌺x
\end_layout

\begin_layout Verbatim

   w+.×⍨,[2+⍳3]3 3⌺∆Z}
\end_layout

\begin_layout Verbatim

  ∆CC←{x←⍺⊃Z ⋄ ∆z←⍵ ⋄ d←-⌊2÷⍨2↑(⍴x)-⍴∆z ⋄ (⊃d)⊖(1⊃d)⌽[1](⍴x)↑∆z}
\end_layout

\begin_layout Verbatim

  ∆MX←{x←⍺⊃Z ⋄ ∆z←⍵ ⋄ y×x=y←(⍴x)↑2⌿2/[1]∆z}
\end_layout

\begin_layout Verbatim

  ∆UP←{w←⍺⊃W ⋄ x←⍺⊃Z ⋄ ∆z←⍵ ⋄ _←⍺ ∆(⍉,[⍳2]x)+.×,[⍳2](2 2⍴2)⌺∆z
\end_layout

\begin_layout Verbatim

   (,[2+⍳3](2 2⍴2)⌺∆z)+.×⍉⍪w}
\end_layout

\begin_layout Verbatim

  ∆C1←{w←⍺⊃W ⋄ x←⍺⊃Z ⋄ ∆z←⍵ ⋄ _←⍺ ∆(⍉,[⍳2]x)+.×,[⍳2]∆z ⋄ ∆z+.×⍉w}
\end_layout

\begin_layout Verbatim

  ∆LA←{⍺≥≢Z:⍵
\end_layout

\begin_layout Verbatim

   down←(⍺+6)∇(⍺+3)∆CV(⍺+4)∆CV(⍺+5)∆UP ⍵↑[2]⍨-2÷⍨⊃⌽⍴⍵
\end_layout

\begin_layout Verbatim

   (⍺+0)∆CV(⍺+1)∆CV(⍵ ∆CC⍨⍺+2)+(⍺+2)∆MX down}
\end_layout

\begin_layout Verbatim

  3 ∆LA 0 ∆CV 1 ∆CV 2 ∆C1 Y∆-(~Y),[1.5]Y}
\end_layout

\begin_layout Verbatim

\end_layout

\begin_layout Verbatim

 E←{-+⌿,⍟(⍺×⍵[;;1])+(~⍺)×⍵[;;0]}
\end_layout

\begin_layout Verbatim

\end_layout

\begin_layout Verbatim

 RUN←{Y Y∆(Y E Y∆)⊣(Y←⌊0.5+nm↑⍵↓⍨2÷⍨(⍴⍵)-nm←2↑⍴Y∆)BCK⊢Y∆←FWD ⍺}
\end_layout

\begin_layout Verbatim

\end_layout

\begin_layout Verbatim

:EndNamespace
\end_layout

\end_body
\end_document
